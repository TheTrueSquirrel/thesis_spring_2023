{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request data from USGS and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "\n",
    "# Define the bounding box for the area of interest\n",
    "min_latitude = 10\n",
    "max_latitude = 60\n",
    "min_longitude = 134 #117 is wide\n",
    "max_longitude = 174 #165 more tight\n",
    "\n",
    "# Create an empty list to hold the earthquake data\n",
    "earthquakes = []\n",
    "\n",
    "for year in range(1973, 2023):\n",
    "    for month in range(1, 13):\n",
    "        # Set the parameters for the API request\n",
    "        starttime = f'{year}-{month:02d}-01'\n",
    "        endtime = f'{year}-{month+1:02d}-01'\n",
    "        if month == 12:\n",
    "            endtime = f'{year+1}-01-01'\n",
    "        params = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': starttime,\n",
    "            'endtime': endtime,\n",
    "            'minmagnitude': '0',\n",
    "            'maxmagnitude': '10',\n",
    "            'minlatitude': min_latitude,\n",
    "            'maxlatitude': max_latitude,\n",
    "            'minlongitude': min_longitude,\n",
    "            'maxlongitude': max_longitude\n",
    "        }\n",
    "\n",
    "        # Send the API request and get the response\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the data for each earthquake and append it to the list\n",
    "        for feature in data['features']:\n",
    "            longitude = feature['geometry']['coordinates'][0]\n",
    "            latitude = feature['geometry']['coordinates'][1]\n",
    "            time = pd.to_datetime(feature['properties']['time'], unit='ms')\n",
    "            magnitude = feature['properties']['mag']\n",
    "            earthquake = {'Longitude': longitude, 'Latitude': latitude, 'Time': time, 'Magnitude': magnitude}\n",
    "            earthquakes.append(earthquake)\n",
    "\n",
    "# Create a DataFrame from the list of earthquake data\n",
    "df = pd.DataFrame(earthquakes)\n",
    "\n",
    "# Cut off magnitudes of 0\n",
    "df = df[df.Magnitude > 0]\n",
    "\n",
    "# save as csv\n",
    "df.to_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape, df.Magnitude.min())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSV to 3D numpy array (where D3 are bins of magnitude) and save .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 week in this case)\n",
    "window_size = timedelta(weeks=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Define magnitude bin size and number of bins\n",
    "num_mag_bins = 10\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows, num_mag_bins), dtype=int)\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            \n",
    "            # Bin magnitudes between 0 and 10 and count number of earthquakes in each bin\n",
    "            magnitudes = bin_data['Magnitude']\n",
    "            counts, _ = np.histogram(magnitudes, bins=np.linspace(0, 10, num_mag_bins+1))\n",
    "\n",
    "            # Store counts in tensor\n",
    "            bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "            tensor[bin_idx, i, :] = counts\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "\n",
    "# Print tensor shape\n",
    "np.save('data/Japan_10_60_134_174_1973_2023.npy',tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CSV to 2D numpy array where rows are 2x2 degrees pixels and columns are bins of time of a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            # Check if there are any earthquakes in the current bin\n",
    "            if not bin_data.empty:\n",
    "                # Find the maximum magnitude in the current bin\n",
    "                max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "                # Store maximum magnitude in tensor\n",
    "                bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "                tensor[bin_idx, i] = max_mag\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 5\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Create DataFrame to store bin information\n",
    "bins_df = pd.DataFrame(columns=['Longitude', 'Latitude'])\n",
    "\n",
    "# Iterate over spatial bins\n",
    "for lon in range(min_lon, max_lon, bin_size):\n",
    "    for lat in range(min_lat, max_lat, bin_size):\n",
    "        # Add bin information to DataFrame\n",
    "        bins_df = pd.concat([bins_df, pd.DataFrame({'Longitude': lon, 'Latitude': lat}, index=[0])], ignore_index=True)\n",
    "\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for j in range(num_bins):\n",
    "        lon = bins_df.loc[j, 'Longitude']\n",
    "        lat = bins_df.loc[j, 'Latitude']\n",
    "        \n",
    "        # Get earthquake data within current spatial bin\n",
    "        bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                               (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "        # Check if there are any earthquakes in the current bin\n",
    "        if not bin_data.empty:\n",
    "            # Find the maximum magnitude in the current bin\n",
    "            max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "            # Store maximum magnitude in tensor\n",
    "            bin_idx = j\n",
    "            tensor[bin_idx, i] = max_mag\n",
    "\n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "# Add longitude and latitude columns to DataFrame\n",
    "bins_df['Longitude'] += bin_size/2\n",
    "bins_df['Latitude'] += bin_size/2\n",
    "\n",
    "print(bins_df.head())\n",
    "print(tensor.shape)\n",
    "df = pd.concat((pd.DataFrame(tensor), bins_df), axis=1)\n",
    "# df.to_csv('data/Japan_10_60_134_174_D_5X5_1973_2023.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as crs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').sort_values('Magnitude')\n",
    "# Create a map using Cartopy to display earthquake data with magnitudes, longitude, and latitude\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=crs.Mercator())\n",
    "ax.add_feature(cartopy.feature.LAND, facecolor=[.8,.8,.8])\n",
    "ax.add_feature(cartopy.feature.OCEAN, facecolor=[.95,.95,.95])\n",
    "ax.add_feature(cartopy.feature.COASTLINE,linewidth=0.3)\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle=':',linewidth=0.3)\n",
    "\n",
    "# Add gridlines\n",
    "lon = np.linspace(-180,180,181)\n",
    "lat = np.linspace(-90,90,91)\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "gl.xlocator = mticker.FixedLocator(lon)\n",
    "gl.ylocator = mticker.FixedLocator(lat)\n",
    "gl.top_labels = gl.right_labels = False\n",
    "gl.rotate_labels = True\n",
    "#gl.xlabel_style = {'rotation': 45}\n",
    "\n",
    "# Add coastlines\n",
    "ax.coastlines(color='black', linewidth=0.5)\n",
    "\n",
    "# Plot the earthquake data as scatter points\n",
    "sc = ax.scatter(df['Longitude'], df['Latitude'], c=df['Magnitude'], cmap=\"inferno\", s=np.exp(df['Magnitude'])/100, transform=crs.PlateCarree())\n",
    "\n",
    "# Set the colorbar and its label\n",
    "cbar = fig.colorbar(sc, ax=ax, fraction=0.04, pad=0.02)\n",
    "cbar.set_label('Magnitude')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Earthquakes between 1973 and 2023')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "# Set the bounds of the map to the minimum and maximum longitude and latitude values\n",
    "# Determine the minimum and maximum longitude and latitude values\n",
    "min_lon, max_lon = df['Longitude'].min(), df['Longitude'].max()\n",
    "min_lat, max_lat = df['Latitude'].min(), df['Latitude'].max()\n",
    "ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=crs.PlateCarree())\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plt.scatter(df.Time, df.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2D histogram of amount of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').iloc[:,4:]\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(4.8,6)})\n",
    "g = sns.histplot(\n",
    "    df, x=\"Longitude\", y=\"Latitude\",\n",
    "    bins=(20,25), cbar=True)\n",
    "\n",
    "g.set_xticks(ticks=np.linspace(134, 174, 21), labels=np.linspace(134, 174, 21).astype(int), rotation = 90)\n",
    "\n",
    "g.set_yticks(ticks=np.linspace(10, 60, 26), labels=np.linspace(10, 60, 26).astype(int))\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the most active 2x2 area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "pixel = df[(df.Longitude > 140) & (df.Longitude < 142) & (df.Latitude > 36) & (df.Latitude < 38)]\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.scatter(pixel.Time, pixel.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.jointplot(\n",
    "    data=df, x=\"Time\", y=\"Magnitude\", s=1, marginal_ticks=True, marginal_kws=dict(bins=74)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlation between pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_1973_2023.csv').iloc[:,:-2]\n",
    "df = df[(df > 0).any(axis=1)]\n",
    "df = df.rolling(14,axis=1, center=True, min_periods=0).mean()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.matshow(np.corrcoef(df),0, cmap='seismic',vmin=-1, vmax=1)\n",
    "plt.xlabel(\"2x2 grid pixel\")\n",
    "plt.ylabel(\"2x2 grid pixel\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ARIMA on 10x10 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# load earthquake data for a specific area\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data = data[(data.Longitude > 140) & (data.Longitude < 145) & (data.Latitude > 35) & (data.Latitude < 40)]\n",
    "data.set_index('Time', inplace=True)\n",
    "data = data['Magnitude'].resample('M').max()  # resample by day and get the maximum magnitude of the day\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# fit an ARIMA model to get the summary\n",
    "model = ARIMA(data, order=(3, 0, 3))  # (p, d, q) order\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\"\"\"\n",
    "\n",
    "train_size = 12 * 2\n",
    "total_splits = len(data)-train_size\n",
    "test_size = 1\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=total_splits, max_train_size=train_size ,test_size=test_size)\n",
    "\n",
    "mae_total = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "mag = 6\n",
    "\n",
    "for train_index, test_index in cv.split(data):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "    # fit an ARIMA model\n",
    "    model = ARIMA(data[train_index], order=(1, 1, 0))  # (p, d, q) order\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # forecast next week's magnitudes\n",
    "    forecast = model_fit.forecast(steps=test_size)\n",
    "    # print('true:', data[test_index][0], 'prediction:', round(forecast[0],1))\n",
    "\n",
    "    # evaluate model performance\n",
    "    mae = mean_absolute_error(data[test_index], forecast)\n",
    "    mae_total += mae\n",
    "    #print('inermediate MSE:', mse)\n",
    "\n",
    "    if data[test_index][0] >= mag:\n",
    "        y_true.append(1)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            TP += 1\n",
    "        if forecast[0] < mag:\n",
    "            FN += 1\n",
    "            y_pred.append(0)\n",
    "    if data[test_index][0] < mag:\n",
    "        y_true.append(0)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            FP += 1\n",
    "        if forecast[0] < mag:\n",
    "            y_pred.append(0)\n",
    "            TN += 1\n",
    "\n",
    "acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "precision = TP / (TP+FP)\n",
    "recall = TP / (TP+FN)\n",
    "specificity = TN / (TN+FP)\n",
    "\n",
    "print('accuracy:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity:', specificity)\n",
    "print('Mean Absolute Error:', mae_total/total_splits)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = ['M<6','M>=6']\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualise ACF to define MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_val = acf(data)\n",
    "plt.bar(range(0,len(acf_val)),acf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize PACF to define AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_val = pacf(data)\n",
    "plt.bar(range(0,len(pacf_val)),pacf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise distribution of magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_5X5_1973_2023.csv').iloc[:,:-2]\n",
    "print(np.unique(df))\n",
    "print(\"mean:\", np.mean(np.array(df)[df > 0]))\n",
    "\n",
    "X = 6\n",
    "print(\"percentage of values bigger than X:\", (np.array(df) >= X).sum() / len(np.array(df).flatten()))\n",
    "sns.histplot(np.array(df).flatten(), bins=90, log_scale=(False,True))\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of earthquake magnitudes')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM encoder-decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 53)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose frequency and binsize\n",
    "freq = 'M'\n",
    "binsize = 5\n",
    "\n",
    "# Load earthquake data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').iloc[:,4:]\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "# Bin the longitude and latitude values into 2x2 degree bins\n",
    "df['Longitude_bin'] = pd.cut(df['Longitude'], bins=np.arange(134, 175, binsize))\n",
    "df['Latitude_bin'] = pd.cut(df['Latitude'], bins=np.arange(10, 61, binsize))\n",
    "\n",
    "# Group the data by longitude bin, latitude bin, and day, and compute the maximum magnitude within each group\n",
    "grouped = df.groupby(['Longitude_bin', 'Latitude_bin', pd.Grouper(freq=freq, level='Time')])['Magnitude'].max()\n",
    "\n",
    "# Convert the resulting data to a DataFrame, filling missing values with 0\n",
    "grouped_df = grouped.unstack().fillna(0)\n",
    "\n",
    "# Reshape the resulting data into a tensor with shape (1, time, rows, cols, channels)\n",
    "time = len(grouped_df.columns)\n",
    "rows = len(grouped_df.index.levels[0])\n",
    "cols = len(grouped_df.index.levels[1])\n",
    "dataset = np.zeros((1, time, rows, cols, 1))\n",
    "\n",
    "for t in range(time):\n",
    "    dataset[0, t, :, :, 0] = grouped_df.iloc[:, t].values.reshape(rows, cols)\n",
    "\n",
    "# Rotate dimensions corresponding to 20 and 25, 90 degrees anti-clockwise\n",
    "dataset = np.transpose(dataset, axes=(0, 1, 3, 2, 4))\n",
    "dataset = np.flip(dataset, axis=2)\n",
    "\n",
    "dataset = dataset.reshape((dataset.shape[1], -1))\n",
    "dataset = dataset[:, dataset.any(axis=0)]\n",
    "# Print the shape of the resulting tensor\n",
    "print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# define magnitude cutoff\n",
    "mag = 4.5\n",
    "mag_select = (dataset >= mag)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split data in train en test set\n",
    "train, test = train_test_split(dataset, test_size=.3, shuffle=False, random_state=43)\n",
    "mag_train, mag_test = train_test_split(mag_select, test_size=.3, shuffle=False, random_state=43)\n",
    "\n",
    "# define generator\n",
    "n_features = dataset.shape[1]\n",
    "n_input = 10\n",
    "steps_epoch = 32\n",
    "train_generator = TimeseriesGenerator(train, mag_train.astype(int), length=n_input, batch_size=(len(train) - n_input) // steps_epoch, shuffle=False)\n",
    "test_generator = TimeseriesGenerator(test, mag_test.astype(int), length=n_input, batch_size=(len(test) - n_input) // steps_epoch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate one step problem with lstm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Flatten, Input, TimeDistributed, Dropout, RepeatVector, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "#########################\n",
    "\"\"\"\n",
    "input= Input(shape=(n_input, n_features))\n",
    "\n",
    "lstm1 = LSTM(12,return_state=True)\n",
    "LSTM_output, state_h, state_c = lstm1(input) \n",
    "states = [state_h, state_c]\n",
    "\n",
    "repeat=RepeatVector(1)\n",
    "LSTM_output = repeat(LSTM_output)\n",
    "\n",
    "lstm2 = LSTM(12,return_sequences=True)\n",
    "all_state_h = lstm2(LSTM_output,initial_state=states)\n",
    "\n",
    "\n",
    "dense = TimeDistributed(Dense(n_features, activation='sigmoid'))\n",
    "output = dense(all_state_h)\n",
    "model_LSTM_return_state = Model(input,output,name='model_LSTM_return_state')\n",
    "\n",
    "model_LSTM_return_state.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "########################\n",
    "#\n",
    "encoder_inputs = Input(shape=(n_input, n_features))\n",
    "encoder_outputs1 = LSTM(12,return_sequences = True, return_state=True, activation='tanh')(encoder_inputs)\n",
    "encoder_states1 = encoder_outputs1[1:]\n",
    "encoder_outputs2 = LSTM(12, return_state=True, activation='tanh')(encoder_outputs1[0])\n",
    "\n",
    "encoder_states2 = encoder_outputs2[1:]\n",
    "#\n",
    "decoder_inputs = RepeatVector(1)(encoder_outputs2[0])\n",
    "#\n",
    "decoder_l1 = LSTM(12, return_sequences=True, activation='tanh')(decoder_inputs,initial_state = encoder_states1)\n",
    "decoder_l2 = LSTM(12, return_sequences=True, activation='tanh')(decoder_l1,initial_state = encoder_states2)\n",
    "decoder_outputs2 = TimeDistributed(Dense(n_features, activation='sigmoid'))(decoder_l2)\n",
    "#\n",
    "model = Model(encoder_inputs,decoder_outputs2)\n",
    "#\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "print(model.summary())\n",
    "\"\"\"\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(12, activation='tanh', return_sequences=True, input_shape=(n_input, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(12, activation='tanh', return_sequences=True, input_shape=(n_input, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(12, activation='tanh', input_shape=(n_input, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(n_features, activation='sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[keras.metrics.Precision(), keras.metrics.Recall(), 'accuracy'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "32/32 [==============================] - 3s 28ms/step - loss: 0.7776 - precision: 0.3029 - recall: 0.5070 - accuracy: 0.0107 - val_loss: 0.6636 - val_precision: 0.5541 - val_recall: 0.6324 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6965 - precision: 0.3466 - recall: 0.5548 - accuracy: 0.0187 - val_loss: 0.6360 - val_precision: 0.6241 - val_recall: 0.6498 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6377 - precision: 0.4202 - recall: 0.6115 - accuracy: 0.0267 - val_loss: 0.5945 - val_precision: 0.7211 - val_recall: 0.7509 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.5689 - precision: 0.5448 - recall: 0.6900 - accuracy: 0.0187 - val_loss: 0.5324 - val_precision: 0.7833 - val_recall: 0.7427 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.4890 - precision: 0.6891 - recall: 0.7529 - accuracy: 0.0053 - val_loss: 0.4601 - val_precision: 0.8055 - val_recall: 0.7307 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.4070 - precision: 0.7750 - recall: 0.7868 - accuracy: 0.0053 - val_loss: 0.3990 - val_precision: 0.8081 - val_recall: 0.7519 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3423 - precision: 0.8047 - recall: 0.8039 - accuracy: 0.0027 - val_loss: 0.3492 - val_precision: 0.8155 - val_recall: 0.7543 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3013 - precision: 0.8101 - recall: 0.8065 - accuracy: 0.0027 - val_loss: 0.3262 - val_precision: 0.8206 - val_recall: 0.7478 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2780 - precision: 0.8137 - recall: 0.8121 - accuracy: 0.0000e+00 - val_loss: 0.3165 - val_precision: 0.8226 - val_recall: 0.7468 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2649 - precision: 0.8186 - recall: 0.8078 - accuracy: 0.0000e+00 - val_loss: 0.3134 - val_precision: 0.8301 - val_recall: 0.7355 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2535 - precision: 0.8245 - recall: 0.8147 - accuracy: 0.0027 - val_loss: 0.3131 - val_precision: 0.8266 - val_recall: 0.7498 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2479 - precision: 0.8252 - recall: 0.8157 - accuracy: 0.0000e+00 - val_loss: 0.3137 - val_precision: 0.8370 - val_recall: 0.7396 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.2449 - precision: 0.8268 - recall: 0.8167 - accuracy: 0.0027 - val_loss: 0.3170 - val_precision: 0.8392 - val_recall: 0.7338 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2403 - precision: 0.8302 - recall: 0.8148 - accuracy: 0.0026 - val_loss: 0.3215 - val_precision: 0.8348 - val_recall: 0.7348 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2389 - precision: 0.8329 - recall: 0.8180 - accuracy: 0.0027 - val_loss: 0.3225 - val_precision: 0.8369 - val_recall: 0.7287 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2300 - precision: 0.8359 - recall: 0.8279 - accuracy: 0.0027 - val_loss: 0.3282 - val_precision: 0.8335 - val_recall: 0.7242 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# early stopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# fit model\n",
    "history = model.fit(train_generator, validation_data=test_generator, steps_per_epoch=steps_epoch, epochs=1000, verbose=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "y_test = scaler.inverse_transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.765594\n",
      "Precision: 0.581514\n",
      "Recall: 0.979045\n",
      "F1 score: 0.729647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         M<6       0.99      0.66      0.79      6099\n",
      "        M>=6       0.58      0.98      0.73      2911\n",
      "\n",
      "    accuracy                           0.77      9010\n",
      "   macro avg       0.78      0.82      0.76      9010\n",
      "weighted avg       0.85      0.77      0.77      9010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "class_names = ['M<6', 'M>=6']\n",
    "\n",
    "print(classification_report(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'M<6'), Text(0, 1.5, 'M>6')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p = sns.heatmap(confusion_matrix(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5), annot=True, fmt='g')\n",
    "p.set_xlabel(\"Predicted\")\n",
    "p.set_ylabel(\"True\")\n",
    "p.xaxis.set_ticklabels(['M<6', 'M>6'], ha=\"center\", va=\"center\")\n",
    "p.yaxis.set_ticklabels(['M<6', 'M>6'], rotation=0, va=\"center\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_ROC_AUC(X,y):\n",
    "\n",
    "    # Use the trained model to predict the class probabilities for the validation set\n",
    "    y_prob = model.predict(X)\n",
    "    y_pred = scaler.inverse_transform(y_prob)\n",
    "    y_test = scaler.inverse_transform(y)\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_test[:-n_input].flatten() >= mag, y_pred.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
    "            ''.format(roc_auc), color='blue', linewidth=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Test)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig(savefig)\n",
    "    plt.show()\n",
    "\n",
    "plot_ROC_AUC(test_generator, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 5D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose frequency and binsize\n",
    "freq = 'M'\n",
    "binsize = 5\n",
    "\n",
    "# Load earthquake data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').iloc[:,4:]\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "# Bin the longitude and latitude values into 2x2 degree bins\n",
    "df['Longitude_bin'] = pd.cut(df['Longitude'], bins=np.arange(134, 175, binsize))\n",
    "df['Latitude_bin'] = pd.cut(df['Latitude'], bins=np.arange(10, 61, binsize))\n",
    "\n",
    "# Group the data by longitude bin, latitude bin, and day, and compute the maximum magnitude within each group\n",
    "grouped = df.groupby(['Longitude_bin', 'Latitude_bin', pd.Grouper(freq=freq, level='Time')])['Magnitude'].max()\n",
    "\n",
    "# Convert the resulting data to a DataFrame, filling missing values with 0\n",
    "grouped_df = grouped.unstack().fillna(0)\n",
    "\n",
    "# Reshape the resulting data into a tensor with shape (1, time, rows, cols, channels)\n",
    "time = len(grouped_df.columns)\n",
    "rows = len(grouped_df.index.levels[0])\n",
    "cols = len(grouped_df.index.levels[1])\n",
    "tensor_convLSTM = np.zeros((1, time, rows, cols, 1))\n",
    "\n",
    "for t in range(time):\n",
    "    tensor_convLSTM[0, t, :, :, 0] = grouped_df.iloc[:, t].values.reshape(rows, cols)\n",
    "\n",
    "# Rotate dimensions corresponding to 20 and 25, 90 degrees anti-clockwise\n",
    "tensor_convLSTM = np.transpose(tensor_convLSTM, axes=(0, 1, 3, 2, 4))\n",
    "tensor_convLSTM = np.flip(tensor_convLSTM, axis=2)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "print(tensor_convLSTM.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot timesteps of 5D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose a timestep to plot (e.g. the first timestep)\n",
    "timestep = 400\n",
    "\n",
    "# Extract the data for the chosen timestep from the tensor\n",
    "data = tensor_convLSTM[0, timestep, :, :, 0]\n",
    "\n",
    "# Create a heatmap plot of the data using Seaborn\n",
    "sns.set(rc={'figure.figsize':(4.8,6)})\n",
    "sns.heatmap(data, cmap='viridis', vmin=-1, vmax=10, linewidths=0.5, linecolor='grey', annot=False)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(f'Earthquake magnitudes at timestep {timestep}')\n",
    "plt.xlabel('Longitude bin')\n",
    "plt.ylabel('Latitude bin')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data in train en test set\n",
    "dataset_convLSTM = tensor_convLSTM.reshape((tensor_convLSTM.shape[1], tensor_convLSTM.shape[2], tensor_convLSTM.shape[3], tensor_convLSTM.shape[4]))\n",
    "\n",
    "train, val_test = train_test_split(dataset_convLSTM, test_size=.4, shuffle=False, random_state=43)\n",
    "val, test = train_test_split(val_test, test_size=.5, shuffle=False, random_state=43)\n",
    "\n",
    "\"\"\"\n",
    "train = train.reshape((1, train.shape[0], train.shape[1], train.shape[2], train.shape[3]))\n",
    "val = val.reshape((1, val.shape[0], val.shape[1], val.shape[2], val.shape[3]))\n",
    "test = test.reshape((1, test.shape[0], test.shape[1], test.shape[2], test.shape[3]))\n",
    "\n",
    "# We'll define a helper function to shift the frames, where `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
    "def create_shifted_frames(data):\n",
    "    x = data[:, 0 : data.shape[1] - 1, :, :]\n",
    "    y = data[:, 1 : data.shape[1], :, :]\n",
    "    return x, y\n",
    "\n",
    "# Apply the processing function to the datasets.\n",
    "x_train, y_train = create_shifted_frames(train)\n",
    "x_val, y_val = create_shifted_frames(val)\n",
    "x_test, y_test = create_shifted_frames(test)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets from timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import tensorflow as tf\n",
    "\n",
    "def dataset_generator(data, seq_length, cutoff):\n",
    "\n",
    "  input_data = data # data[:-seq_length]\n",
    "  targets = data[seq_length:]\n",
    "  dataset = timeseries_dataset_from_array(input_data, (targets >= cutoff).astype(int), sequence_length=seq_length, sampling_rate=1, sequence_stride=1, shuffle=False, batch_size=len(data))\n",
    "  \"\"\"\n",
    "  for batch in dataset:\n",
    "    inputs, targets = batch\n",
    "    assert np.array_equal(inputs[0], data[:seq_length])  # First sequence: steps [0-9]\n",
    "    assert np.array_equal(targets[0], data[seq_length])  # Corresponding target: step 10\n",
    "    \"\"\"\n",
    "  return dataset\n",
    "\n",
    "# Set lookback timewindow\n",
    "timewindow = 10\n",
    "\n",
    "train_dataset = dataset_generator(train, timewindow, 4.5)\n",
    "val_dataset = dataset_generator(val, timewindow, 4.5)\n",
    "test_dataset = dataset_generator(test, timewindow, 4.5)\n",
    "\n",
    "# Create train set\n",
    "for batch in train_dataset:\n",
    "    X_train, y_train = batch\n",
    "\n",
    "y_train = tf.reshape(y_train, shape=[y_train.shape[0], 1, y_train.shape[1], y_train.shape[2], y_train.shape[3]])\n",
    "\n",
    "# Create validation set\n",
    "for batch in val_dataset:\n",
    "    X_val, y_val = batch\n",
    "\n",
    "y_val = tf.reshape(y_val, shape=[y_val.shape[0], 1, y_val.shape[1], y_val.shape[2], y_val.shape[3]])\n",
    "\n",
    "# Create test set\n",
    "for batch in test_dataset:\n",
    "    X_test, y_test = batch\n",
    "\n",
    "y_test = tf.reshape(y_test, shape=[y_test.shape[0], 1, y_test.shape[1], y_test.shape[2], y_test.shape[3]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelconstruction of convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define encoder\n",
    "encoder_inputs = layers.Input(shape=(X_train.shape[1:]))\n",
    "encoder = layers.BatchNormalization()(encoder_inputs)\n",
    "encoder, state_h_1, state_c_1 = layers.ConvLSTM2D(\n",
    "    filters=12,\n",
    "    kernel_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    activation=\"tanh\"\n",
    ")(encoder)\n",
    "encoder = layers.BatchNormalization()(encoder)\n",
    "encoder, state_h_2, state_c_2 = layers.ConvLSTM2D(\n",
    "    filters=12,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=False,\n",
    "    return_state=True,\n",
    "    activation=\"tanh\"\n",
    ")(encoder)\n",
    "encoder = layers.BatchNormalization()(encoder)\n",
    "\n",
    "encoder_states = [state_h_1, state_c_1, state_h_2, state_c_2]\n",
    "\n",
    "# Define decoder\n",
    "decoder_inputs = layers.Input(shape=encoder.shape[1:])\n",
    "decoder = layers.Reshape((1, *decoder_inputs.shape[1:]))(decoder_inputs)\n",
    "\n",
    "decoder_lstm = layers.ConvLSTM2D(\n",
    "    filters=12,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"tanh\"\n",
    ")\n",
    "\n",
    "decoder = decoder_lstm(decoder, initial_state=encoder_states[:2])\n",
    "decoder = layers.BatchNormalization()(decoder)\n",
    "decoder = layers.ConvLSTM2D(\n",
    "    filters=12,\n",
    "    kernel_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"tanh\"\n",
    ")(decoder, initial_state=encoder_states[2:])\n",
    "decoder = layers.BatchNormalization()(decoder)\n",
    "\n",
    "decoder_outputs = layers.Conv3D(filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    ")(decoder)\n",
    "\n",
    "# Define model\n",
    "model = keras.models.Model(encoder_inputs, decoder_outputs)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.0005), metrics=[keras.metrics.Precision(), keras.metrics.Recall(), 'accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeltraining of convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some callbacks to improve training.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3)\n",
    "\n",
    "# Define modifiable training hyperparameters.\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Fit the model to the training data.\n",
    "model.fit(x=X_train,\n",
    "          y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Choose timesteps to plot\n",
    "timestep = 20\n",
    "\n",
    "# Extract the data for the chosen timesteps from the tensor\n",
    "data1 = y_pred[timestep, 0, :, :, 0] > .5\n",
    "data2 = y_test[timestep, 0, :, :, 0]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plot the data in each subplot\n",
    "sns.heatmap(data1, cmap='viridis', vmin=-1, vmax=10, linewidths=0.5, linecolor='grey', annot=False, ax=ax1)\n",
    "sns.heatmap(data2, cmap='viridis', vmin=-1, vmax=10, linewidths=0.5, linecolor='grey', annot=False, ax=ax2)\n",
    "\n",
    "# Set the plot titles and axis labels\n",
    "ax1.set_title(f'Earthquake magnitudes at timestep {timestep}')\n",
    "ax1.set_xlabel('Longitude bin')\n",
    "ax1.set_ylabel('Latitude bin')\n",
    "\n",
    "ax2.set_title(f'Earthquake magnitudes at timestep {timestep}')\n",
    "ax2.set_xlabel('Longitude bin')\n",
    "ax2.set_ylabel('Latitude bin')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p = sns.heatmap(confusion_matrix(np.array(y_test).flatten(), y_pred.flatten() >= 0.5), annot=True, fmt='g')\n",
    "p.set_xlabel(\"Predicted\")\n",
    "p.set_ylabel(\"True\")\n",
    "p.xaxis.set_ticklabels(['M<6', 'M>=6'], ha=\"center\", va=\"center\")\n",
    "p.yaxis.set_ticklabels(['M<6', 'M>=6'], rotation=0, va=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(np.array(data2).flatten(), data1.flatten())\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(np.array(data2).flatten(), data1.flatten())\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(np.array(data2).flatten(), data1.flatten())\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(np.array(data2).flatten(), data1.flatten())\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "class_names = ['M<6', 'M>=6']\n",
    "\n",
    "print(classification_report(np.array(data2).flatten(), data1.flatten(), target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hereafter is mainly redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat = pd.read_excel(\"/Users/jurrienboogert/Downloads/emdat_public_2023_02_08_query_uid-jHccdA.xlsx\", header=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat[emdat['Disaster Type'] == 'Earthquake'][['Dis Mag Value']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file='/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/NOAA/earthquakes-2023-02-11_10-24-26_+0100.tsv'\n",
    "NOAA=pd.read_table(tsv_file,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag']\n",
    "\n",
    "plt.hist(NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'], bins=80)\n",
    "plt.show()\n",
    "\n",
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'].quantile(.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(NOAA[NOAA['Year'] > 2010][['year', 'month', 'day', 'hour', 'minute']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk1.csv\", low_memory=False)\n",
    "chunk2 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk2.csv\", low_memory=False)\n",
    "chunk3 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk3.csv\", low_memory=False)\n",
    "chunk4 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk4.csv\", low_memory=False)\n",
    "chunk5 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk5.csv\", low_memory=False)\n",
    "chunk6 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk6.csv\", low_memory=False)\n",
    "\n",
    "chunks = pd.concat([chunk2,chunk3,chunk4,chunk5,chunk6], ignore_index=True)\n",
    "chunks['trace_start_time'] = pd.to_datetime(chunks['trace_start_time'])\n",
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "chunks = chunks.sort_values('source_origin_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset = chunks[(chunks['source_longitude'] >= 19) & (chunks['source_longitude'] <= 30) & (chunks['source_latitude'] >= 34) & (chunks['source_latitude'] <= 44) & (chunks['source_origin_time'] <= '2015-6-25 03:14:47.900')]\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "plt.scatter(subset.source_origin_time, subset.source_magnitude, s=1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(subset.source_magnitude, bins=48)\n",
    "plt.xlabel('magnitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a histogram of the longitude values\n",
    "plt.hist(subset.source_latitude, bins=360)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Longitude')\n",
    "plt.show()\n",
    "\n",
    "# create a histogram of the latitude values\n",
    "plt.hist(subset.source_longitude, bins=360)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "\n",
    "# Load earthquake data\n",
    "df = subset[chunks['source_magnitude'] >= 0].sort_values('source_magnitude')\n",
    "\n",
    "# Extract latitude and longitude columns\n",
    "latitudes = df['source_latitude']\n",
    "longitudes = df['source_longitude']\n",
    "magnitudes = df['source_magnitude']\n",
    "\n",
    "# Set up map projection\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "map = Basemap(projection='merc', lat_0=0, lon_0=0, resolution='l',\n",
    "              llcrnrlon=19, llcrnrlat=33, urcrnrlon=30, urcrnrlat=43)\n",
    "\n",
    "# Draw coastlines, countries, and states\n",
    "#map.drawcoastlines(color='gray')\n",
    "map.fillcontinents(color='lightgray', lake_color='white')\n",
    "map.drawmapboundary(fill_color='white')\n",
    "\n",
    "# Draw parallels and meridians\n",
    "map.drawparallels(range(-90, 90, 1), linewidth=0.5, labels=[1, 0, 0, 0])\n",
    "meridians = map.drawmeridians(range(-180, 180, 1), linewidth=0.5, labels=[0, 0, 0, 1])\n",
    "\n",
    "for m in meridians:\n",
    "    try:\n",
    "        meridians[m][1][0].set_rotation(45)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Convert latitude and longitude to map coordinates\n",
    "x, y = map(longitudes, latitudes)\n",
    "\n",
    "# Plot earthquake magnitudes as circles on the map\n",
    "map.scatter(x, y, s=np.exp(magnitudes)/50, c=magnitudes, cmap='plasma', alpha=1)\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Earthquake Magnitudes')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# group the data by date and location to create the input data for Prophet\n",
    "data = chunks.groupby([pd.Grouper(key='source_origin_time', freq='D'), 'source_latitude', 'source_longitude']).size().reset_index(name='count')\n",
    "\n",
    "# rename columns for use with Prophet\n",
    "data = data.rename(columns={'source_origin_time': 'ds', 'count': 'y'})\n",
    "\n",
    "# create a Prophet model and fit the data\n",
    "model = Prophet()\n",
    "model.fit(data)\n",
    "\n",
    "# create a future dataframe with predictions for the next 365 days\n",
    "future = model.make_future_dataframe(periods=7)\n",
    "\n",
    "# predict the number of earthquakes for the future dates\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# plot the forecast\n",
    "fig = model.plot(forecast, xlabel='Date', ylabel='Number of Earthquakes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chunks['source_origin_time'], chunks['source_magnitude'], 'ro', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[['source_magnitude', 'trace_start_time', 'source_origin_time', 'receiver_latitude', 'receiver_longitude', 'source_latitude', 'source_longitude']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[(chunks['source_origin_time'].dt.year == 2007) & (chunks['source_origin_time'].dt.month == 8) & (chunks['source_origin_time'].dt.day == 17) & (chunks['source_origin_time'].dt.hour == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[chunks['source_magnitude'] > 6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Year'] > 1983) & (NOAA['Deaths'] > 0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"/Users/jurrienboogert/Downloads/2023.csv\", header=None, names=['ID', 'YEAR/MONTH/DAY', 'ELEMENT', 'DATA VALUE', 'M-FLAG', 'Q-FLAG', 'S-FLAG', 'OBS-TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dly = pd.read_fwf('/Users/jurrienboogert/Downloads/ghcnd_gsn/ghcnd_gsn/USW00013782.dly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = chunks.sample(1)[['source_latitude', 'source_longitude']]\n",
    "source.iloc[0,0], source.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "# Set time period\n",
    "start = datetime(1984, 9, 7, 2)\n",
    "end = datetime(1984, 9, 7, 3)\n",
    "\n",
    "# Create Point for Vancouver, BC\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 200000\n",
    "Point.max_count = 6\n",
    "Point.weight_dist = .6\n",
    "receiver = Point(source.iloc[0,0], source.iloc[0,1])\n",
    "# Get daily data for 2018\n",
    "data = Hourly(receiver, start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "year = chunks['source_origin_time'].dt.year.fillna(0).astype('int')\n",
    "month = chunks['source_origin_time'].dt.month.fillna(0).astype('int')\n",
    "day = chunks['source_origin_time'].dt.day.fillna(0).astype('int')\n",
    "hour = chunks['source_origin_time'].dt.hour.fillna(0).astype('int')\n",
    "\n",
    "chunks.source_latitude\n",
    "chunks.source_longitude\n",
    "\n",
    "chunks.receiver_latitude\n",
    "chunks.receiver_longitude\n",
    "chunks.receiver_elevation_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "temp_source = []\n",
    "rhum_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "rhum_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i], hour[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Hourly(source, start, end).fetch()['temp'][0])\n",
    "    rhum_source.append(Hourly(source, start, end).fetch()['rhum'][0])\n",
    "    pres_source.append(Hourly(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "temp_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Daily(source, start, end).fetch()['tavg'][0])\n",
    "    pres_source.append(Daily(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    Point.alt_range = chunks.receiver_elevation_m[i]\n",
    "    Point.adapt_temp = True\n",
    "    receiver = Point(chunks.receiver_latitude[i], chunks.receiver_longitude[i], chunks.receiver_elevation_m[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_receiver.append(Daily(receiver, start, end).fetch()['tavg'][0])\n",
    "    pres_receiver.append(Daily(receiver, start, end).fetch()['pres'][0])\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8acd2a4c40bb06440d03e583eeea35c6596324a3385dafe16353bbc1939be192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
