{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request data from USGS and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "\n",
    "# Define the bounding box for the area of interest\n",
    "min_latitude = 10\n",
    "max_latitude = 60\n",
    "min_longitude = 134 #117 is wide\n",
    "max_longitude = 174 #165 more tight\n",
    "\n",
    "# Create an empty list to hold the earthquake data\n",
    "earthquakes = []\n",
    "\n",
    "for year in range(1973, 2023):\n",
    "    for month in range(1, 13):\n",
    "        # Set the parameters for the API request\n",
    "        starttime = f'{year}-{month:02d}-01'\n",
    "        endtime = f'{year}-{month+1:02d}-01'\n",
    "        if month == 12:\n",
    "            endtime = f'{year+1}-01-01'\n",
    "        params = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': starttime,\n",
    "            'endtime': endtime,\n",
    "            'minmagnitude': '0',\n",
    "            'maxmagnitude': '10',\n",
    "            'minlatitude': min_latitude,\n",
    "            'maxlatitude': max_latitude,\n",
    "            'minlongitude': min_longitude,\n",
    "            'maxlongitude': max_longitude\n",
    "        }\n",
    "\n",
    "        # Send the API request and get the response\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the data for each earthquake and append it to the list\n",
    "        for feature in data['features']:\n",
    "            longitude = feature['geometry']['coordinates'][0]\n",
    "            latitude = feature['geometry']['coordinates'][1]\n",
    "            time = pd.to_datetime(feature['properties']['time'], unit='ms')\n",
    "            magnitude = feature['properties']['mag']\n",
    "            earthquake = {'Longitude': longitude, 'Latitude': latitude, 'Time': time, 'Magnitude': magnitude}\n",
    "            earthquakes.append(earthquake)\n",
    "\n",
    "# Create a DataFrame from the list of earthquake data\n",
    "df = pd.DataFrame(earthquakes)\n",
    "\n",
    "# Cut off magnitudes of 0\n",
    "df = df[df.Magnitude > 0]\n",
    "\n",
    "# save as csv\n",
    "df.to_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape, df.Magnitude.min())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSV to 3D numpy array (where D3 are bins of magnitude) and save .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 week in this case)\n",
    "window_size = timedelta(weeks=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Define magnitude bin size and number of bins\n",
    "num_mag_bins = 10\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows, num_mag_bins), dtype=int)\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            \n",
    "            # Bin magnitudes between 0 and 10 and count number of earthquakes in each bin\n",
    "            magnitudes = bin_data['Magnitude']\n",
    "            counts, _ = np.histogram(magnitudes, bins=np.linspace(0, 10, num_mag_bins+1))\n",
    "\n",
    "            # Store counts in tensor\n",
    "            bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "            tensor[bin_idx, i, :] = counts\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "\n",
    "# Print tensor shape\n",
    "np.save('data/Japan_10_60_134_174_1973_2023.npy',tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CSV to 2D numpy array where rows are 2x2 degrees pixels and columns are bins of time of a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            # Check if there are any earthquakes in the current bin\n",
    "            if not bin_data.empty:\n",
    "                # Find the maximum magnitude in the current bin\n",
    "                max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "                # Store maximum magnitude in tensor\n",
    "                bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "                tensor[bin_idx, i] = max_mag\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Create DataFrame to store bin information\n",
    "bins_df = pd.DataFrame(columns=['Longitude', 'Latitude'])\n",
    "\n",
    "# Iterate over spatial bins\n",
    "for lon in range(min_lon, max_lon, bin_size):\n",
    "    for lat in range(min_lat, max_lat, bin_size):\n",
    "        # Add bin information to DataFrame\n",
    "        bins_df = pd.concat([bins_df, pd.DataFrame({'Longitude': lon, 'Latitude': lat}, index=[0])], ignore_index=True)\n",
    "\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for j in range(num_bins):\n",
    "        lon = bins_df.loc[j, 'Longitude']\n",
    "        lat = bins_df.loc[j, 'Latitude']\n",
    "        \n",
    "        # Get earthquake data within current spatial bin\n",
    "        bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                               (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "        # Check if there are any earthquakes in the current bin\n",
    "        if not bin_data.empty:\n",
    "            # Find the maximum magnitude in the current bin\n",
    "            max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "            # Store maximum magnitude in tensor\n",
    "            bin_idx = j\n",
    "            tensor[bin_idx, i] = max_mag\n",
    "\n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "# Add longitude and latitude columns to DataFrame\n",
    "bins_df['Longitude'] += bin_size/2\n",
    "bins_df['Latitude'] += bin_size/2\n",
    "\n",
    "print(bins_df.head())\n",
    "print(tensor.shape)\n",
    "df = pd.concat((pd.DataFrame(tensor), bins_df), axis=1)\n",
    "# df.to_csv('data/Japan_10_60_134_174_D_1973_2023.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as crs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').sort_values('Magnitude')\n",
    "# Create a map using Cartopy to display earthquake data with magnitudes, longitude, and latitude\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=crs.Mercator())\n",
    "ax.add_feature(cartopy.feature.LAND, facecolor=[.8,.8,.8])\n",
    "ax.add_feature(cartopy.feature.OCEAN, facecolor=[.95,.95,.95])\n",
    "ax.add_feature(cartopy.feature.COASTLINE,linewidth=0.3)\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle=':',linewidth=0.3)\n",
    "\n",
    "# Add gridlines\n",
    "lon = np.linspace(-180,180,181)\n",
    "lat = np.linspace(-90,90,91)\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "gl.xlocator = mticker.FixedLocator(lon)\n",
    "gl.ylocator = mticker.FixedLocator(lat)\n",
    "gl.top_labels = gl.right_labels = False\n",
    "gl.rotate_labels = True\n",
    "#gl.xlabel_style = {'rotation': 45}\n",
    "\n",
    "# Add coastlines\n",
    "ax.coastlines(color='black', linewidth=0.5)\n",
    "\n",
    "# Plot the earthquake data as scatter points\n",
    "sc = ax.scatter(df['Longitude'], df['Latitude'], c=df['Magnitude'], cmap=\"inferno\", s=np.exp(df['Magnitude'])/100, transform=crs.PlateCarree())\n",
    "\n",
    "# Set the colorbar and its label\n",
    "cbar = fig.colorbar(sc, ax=ax, fraction=0.04, pad=0.02)\n",
    "cbar.set_label('Magnitude')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Earthquakes between 1973 and 2023')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "# Set the bounds of the map to the minimum and maximum longitude and latitude values\n",
    "# Determine the minimum and maximum longitude and latitude values\n",
    "min_lon, max_lon = df['Longitude'].min(), df['Longitude'].max()\n",
    "min_lat, max_lat = df['Latitude'].min(), df['Latitude'].max()\n",
    "ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=crs.PlateCarree())\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plt.scatter(df.Time, df.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2D histogram of amount of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "g = sns.histplot(\n",
    "    df, x=\"Longitude\", y=\"Latitude\",\n",
    "    bins=(20,25), cbar=True)\n",
    "\n",
    "g.set_xticks(ticks=np.linspace(134, 174, 21), labels=np.linspace(134, 174, 21).astype(int), rotation = 90)\n",
    "\n",
    "g.set_yticks(ticks=np.linspace(10, 60, 26), labels=np.linspace(10, 60, 26).astype(int))\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the most active 2x2 area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "pixel = df[(df.Longitude > 140) & (df.Longitude < 142) & (df.Latitude > 36) & (df.Latitude < 38)]\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.scatter(pixel.Time, pixel.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.jointplot(\n",
    "    data=df, x=\"Time\", y=\"Magnitude\", s=1, marginal_ticks=True, marginal_kws=dict(bins=74)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlation between pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_1973_2023.csv').iloc[:,:-2]\n",
    "df = df[(df > 0).any(axis=1)]\n",
    "df = df.rolling(14,axis=1, center=True, min_periods=0).mean()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.matshow(np.corrcoef(df),0, cmap='seismic',vmin=-1, vmax=1)\n",
    "plt.xlabel(\"2x2 grid pixel\")\n",
    "plt.ylabel(\"2x2 grid pixel\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ARIMA on 2x2 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# load earthquake data for a specific area\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data = data[(data.Longitude > 136) & (data.Longitude < 146) & (data.Latitude > 32) & (data.Latitude < 42)]\n",
    "data.set_index('Time', inplace=True)\n",
    "data = data['Magnitude'].resample('W').max()  # resample by day and get the maximum magnitude of the day\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# fit an ARIMA model to get the summary\n",
    "model = ARIMA(data, order=(3, 0, 3))  # (p, d, q) order\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\"\"\"\n",
    "\n",
    "train_size = 52*20\n",
    "total_splits = len(data)-train_size\n",
    "test_size = 1\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=total_splits, max_train_size=train_size ,test_size=test_size)\n",
    "\n",
    "mae_total = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "mag = 6\n",
    "\n",
    "for train_index, test_index in cv.split(data):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "    # fit an ARIMA model\n",
    "    model = ARIMA(data[train_index], order=(1, 1, 0))  # (p, d, q) order\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # forecast next week's magnitudes\n",
    "    forecast = model_fit.forecast(steps=test_size)\n",
    "    print('true:', data[test_index][0], 'prediction:', round(forecast[0],1))\n",
    "\n",
    "    # evaluate model performance\n",
    "    mae = mean_absolute_error(data[test_index], forecast)\n",
    "    mae_total += mae\n",
    "    #print('inermediate MSE:', mse)\n",
    "\n",
    "    if data[test_index][0] >= mag:\n",
    "        y_true.append(1)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            TP += 1\n",
    "        if forecast[0] < mag:\n",
    "            FN += 1\n",
    "            y_pred.append(0)\n",
    "    if data[test_index][0] < mag:\n",
    "        y_true.append(0)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            FP += 1\n",
    "        if forecast[0] < mag:\n",
    "            y_pred.append(0)\n",
    "            TN += 1\n",
    "\n",
    "acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "precision = TP / (TP+FP)\n",
    "recall = TP / (TP+FN)\n",
    "specificity = TN / (TN+FP)\n",
    "\n",
    "print('accuracy:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity:', specificity)\n",
    "print('Mean Absolute Error:', mae_total/total_splits)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = ['M<6','M>=6']\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualise ACF to define MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_val = acf(data)\n",
    "plt.bar(range(0,len(acf_val)),acf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize PACF to define AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_val = pacf(data)\n",
    "plt.bar(range(0,len(pacf_val)),pacf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "230/230 [==============================] - 4s 8ms/step - loss: 0.7729\n",
      "Epoch 2/500\n",
      "230/230 [==============================] - 2s 8ms/step - loss: 0.6794\n",
      "Epoch 3/500\n",
      "230/230 [==============================] - 2s 8ms/step - loss: 0.6717\n",
      "Epoch 4/500\n",
      "230/230 [==============================] - 2s 8ms/step - loss: 0.6754\n",
      "Epoch 5/500\n",
      "230/230 [==============================] - 2s 8ms/step - loss: 0.6706\n",
      "Epoch 6/500\n",
      "230/230 [==============================] - 2s 8ms/step - loss: 0.6643\n",
      "Epoch 7/500\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.6616\n",
      "Epoch 8/500\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.6495\n",
      "Epoch 9/500\n",
      "230/230 [==============================] - 2s 8ms/step - loss: 0.6411\n",
      "Epoch 10/500\n",
      "115/230 [==============>...............] - ETA: 0s - loss: 0.6271"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(generator, steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m230\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# multivariate one step problem with lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# import data and cut off last two columns of longitude and latitude and transpose\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_1973_2023.csv').iloc[:,:-2].T\n",
    "\n",
    "### resample timeperiod original csv and get Time\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').iloc[:,4:]\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data.set_index('Time', inplace=True)\n",
    "data = data.sort_index()\n",
    "time_D = data.resample('D').max().fillna(0).index\n",
    "\n",
    "df = df.set_index(time_D)\n",
    "\n",
    "# resample time\n",
    "df = df.set_index(time_D).resample('M').max()\n",
    "\n",
    "# only contain columns where at least a mangitude bigger than zero is recorded\n",
    "dataset = df.T[(df > 0).any(axis=0)]\n",
    "\n",
    "\n",
    "# transpose dataset and convert to numpy array\n",
    "dataset = np.array(dataset.T)\n",
    "\n",
    "# scale data\n",
    "# scaler = StandardScaler()\n",
    "# dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# define generator\n",
    "n_features = dataset.shape[1]\n",
    "n_input = 12\n",
    "generator = TimeseriesGenerator(dataset, dataset, length=n_input, batch_size=1)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='tanh', input_shape=(n_input, n_features), return_sequences=True))\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(128, activation='tanh'))\n",
    "model.add(Dense(230, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "# fit model\n",
    "history = model.fit(generator, steps_per_epoch=230, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat = pd.read_excel(\"/Users/jurrienboogert/Downloads/emdat_public_2023_02_08_query_uid-jHccdA.xlsx\", header=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat[emdat['Disaster Type'] == 'Earthquake'][['Dis Mag Value']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file='/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/NOAA/earthquakes-2023-02-11_10-24-26_+0100.tsv'\n",
    "NOAA=pd.read_table(tsv_file,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag']\n",
    "\n",
    "plt.hist(NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'], bins=80)\n",
    "plt.show()\n",
    "\n",
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'].quantile(.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(NOAA[NOAA['Year'] > 2010][['year', 'month', 'day', 'hour', 'minute']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk1.csv\", low_memory=False)\n",
    "chunk2 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk2.csv\", low_memory=False)\n",
    "chunk3 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk3.csv\", low_memory=False)\n",
    "chunk4 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk4.csv\", low_memory=False)\n",
    "chunk5 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk5.csv\", low_memory=False)\n",
    "chunk6 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk6.csv\", low_memory=False)\n",
    "\n",
    "chunks = pd.concat([chunk2,chunk3,chunk4,chunk5,chunk6], ignore_index=True)\n",
    "chunks['trace_start_time'] = pd.to_datetime(chunks['trace_start_time'])\n",
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "chunks = chunks.sort_values('source_origin_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset = chunks[(chunks['source_longitude'] >= 19) & (chunks['source_longitude'] <= 30) & (chunks['source_latitude'] >= 34) & (chunks['source_latitude'] <= 44) & (chunks['source_origin_time'] <= '2015-6-25 03:14:47.900')]\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "plt.scatter(subset.source_origin_time, subset.source_magnitude, s=1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(subset.source_magnitude, bins=48)\n",
    "plt.xlabel('magnitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a histogram of the longitude values\n",
    "plt.hist(subset.source_latitude, bins=360)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Longitude')\n",
    "plt.show()\n",
    "\n",
    "# create a histogram of the latitude values\n",
    "plt.hist(subset.source_longitude, bins=360)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "\n",
    "# Load earthquake data\n",
    "df = subset[chunks['source_magnitude'] >= 0].sort_values('source_magnitude')\n",
    "\n",
    "# Extract latitude and longitude columns\n",
    "latitudes = df['source_latitude']\n",
    "longitudes = df['source_longitude']\n",
    "magnitudes = df['source_magnitude']\n",
    "\n",
    "# Set up map projection\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "map = Basemap(projection='merc', lat_0=0, lon_0=0, resolution='l',\n",
    "              llcrnrlon=19, llcrnrlat=33, urcrnrlon=30, urcrnrlat=43)\n",
    "\n",
    "# Draw coastlines, countries, and states\n",
    "#map.drawcoastlines(color='gray')\n",
    "map.fillcontinents(color='lightgray', lake_color='white')\n",
    "map.drawmapboundary(fill_color='white')\n",
    "\n",
    "# Draw parallels and meridians\n",
    "map.drawparallels(range(-90, 90, 1), linewidth=0.5, labels=[1, 0, 0, 0])\n",
    "meridians = map.drawmeridians(range(-180, 180, 1), linewidth=0.5, labels=[0, 0, 0, 1])\n",
    "\n",
    "for m in meridians:\n",
    "    try:\n",
    "        meridians[m][1][0].set_rotation(45)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Convert latitude and longitude to map coordinates\n",
    "x, y = map(longitudes, latitudes)\n",
    "\n",
    "# Plot earthquake magnitudes as circles on the map\n",
    "map.scatter(x, y, s=np.exp(magnitudes)/50, c=magnitudes, cmap='plasma', alpha=1)\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Earthquake Magnitudes')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# group the data by date and location to create the input data for Prophet\n",
    "data = chunks.groupby([pd.Grouper(key='source_origin_time', freq='D'), 'source_latitude', 'source_longitude']).size().reset_index(name='count')\n",
    "\n",
    "# rename columns for use with Prophet\n",
    "data = data.rename(columns={'source_origin_time': 'ds', 'count': 'y'})\n",
    "\n",
    "# create a Prophet model and fit the data\n",
    "model = Prophet()\n",
    "model.fit(data)\n",
    "\n",
    "# create a future dataframe with predictions for the next 365 days\n",
    "future = model.make_future_dataframe(periods=7)\n",
    "\n",
    "# predict the number of earthquakes for the future dates\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# plot the forecast\n",
    "fig = model.plot(forecast, xlabel='Date', ylabel='Number of Earthquakes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chunks['source_origin_time'], chunks['source_magnitude'], 'ro', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[['source_magnitude', 'trace_start_time', 'source_origin_time', 'receiver_latitude', 'receiver_longitude', 'source_latitude', 'source_longitude']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[(chunks['source_origin_time'].dt.year == 2007) & (chunks['source_origin_time'].dt.month == 8) & (chunks['source_origin_time'].dt.day == 17) & (chunks['source_origin_time'].dt.hour == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[chunks['source_magnitude'] > 6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Year'] > 1983) & (NOAA['Deaths'] > 0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"/Users/jurrienboogert/Downloads/2023.csv\", header=None, names=['ID', 'YEAR/MONTH/DAY', 'ELEMENT', 'DATA VALUE', 'M-FLAG', 'Q-FLAG', 'S-FLAG', 'OBS-TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dly = pd.read_fwf('/Users/jurrienboogert/Downloads/ghcnd_gsn/ghcnd_gsn/USW00013782.dly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = chunks.sample(1)[['source_latitude', 'source_longitude']]\n",
    "source.iloc[0,0], source.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "# Set time period\n",
    "start = datetime(1984, 9, 7, 2)\n",
    "end = datetime(1984, 9, 7, 3)\n",
    "\n",
    "# Create Point for Vancouver, BC\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 200000\n",
    "Point.max_count = 6\n",
    "Point.weight_dist = .6\n",
    "receiver = Point(source.iloc[0,0], source.iloc[0,1])\n",
    "# Get daily data for 2018\n",
    "data = Hourly(receiver, start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "year = chunks['source_origin_time'].dt.year.fillna(0).astype('int')\n",
    "month = chunks['source_origin_time'].dt.month.fillna(0).astype('int')\n",
    "day = chunks['source_origin_time'].dt.day.fillna(0).astype('int')\n",
    "hour = chunks['source_origin_time'].dt.hour.fillna(0).astype('int')\n",
    "\n",
    "chunks.source_latitude\n",
    "chunks.source_longitude\n",
    "\n",
    "chunks.receiver_latitude\n",
    "chunks.receiver_longitude\n",
    "chunks.receiver_elevation_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "temp_source = []\n",
    "rhum_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "rhum_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i], hour[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Hourly(source, start, end).fetch()['temp'][0])\n",
    "    rhum_source.append(Hourly(source, start, end).fetch()['rhum'][0])\n",
    "    pres_source.append(Hourly(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "temp_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Daily(source, start, end).fetch()['tavg'][0])\n",
    "    pres_source.append(Daily(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    Point.alt_range = chunks.receiver_elevation_m[i]\n",
    "    Point.adapt_temp = True\n",
    "    receiver = Point(chunks.receiver_latitude[i], chunks.receiver_longitude[i], chunks.receiver_elevation_m[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_receiver.append(Daily(receiver, start, end).fetch()['tavg'][0])\n",
    "    pres_receiver.append(Daily(receiver, start, end).fetch()['pres'][0])\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8acd2a4c40bb06440d03e583eeea35c6596324a3385dafe16353bbc1939be192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
