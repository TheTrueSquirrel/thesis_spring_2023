{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request data from USGS and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "\n",
    "# Define the bounding box for the area of interest\n",
    "min_latitude = 10\n",
    "max_latitude = 60\n",
    "min_longitude = 134 #117 is wide\n",
    "max_longitude = 174 #165 more tight\n",
    "\n",
    "# Create an empty list to hold the earthquake data\n",
    "earthquakes = []\n",
    "\n",
    "for year in range(1973, 2023):\n",
    "    for month in range(1, 13):\n",
    "        # Set the parameters for the API request\n",
    "        starttime = f'{year}-{month:02d}-01'\n",
    "        endtime = f'{year}-{month+1:02d}-01'\n",
    "        if month == 12:\n",
    "            endtime = f'{year+1}-01-01'\n",
    "        params = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': starttime,\n",
    "            'endtime': endtime,\n",
    "            'minmagnitude': '0',\n",
    "            'maxmagnitude': '10',\n",
    "            'minlatitude': min_latitude,\n",
    "            'maxlatitude': max_latitude,\n",
    "            'minlongitude': min_longitude,\n",
    "            'maxlongitude': max_longitude\n",
    "        }\n",
    "\n",
    "        # Send the API request and get the response\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the data for each earthquake and append it to the list\n",
    "        for feature in data['features']:\n",
    "            longitude = feature['geometry']['coordinates'][0]\n",
    "            latitude = feature['geometry']['coordinates'][1]\n",
    "            time = pd.to_datetime(feature['properties']['time'], unit='ms')\n",
    "            magnitude = feature['properties']['mag']\n",
    "            earthquake = {'Longitude': longitude, 'Latitude': latitude, 'Time': time, 'Magnitude': magnitude}\n",
    "            earthquakes.append(earthquake)\n",
    "\n",
    "# Create a DataFrame from the list of earthquake data\n",
    "df = pd.DataFrame(earthquakes)\n",
    "\n",
    "# Cut off magnitudes of 0\n",
    "df = df[df.Magnitude > 0]\n",
    "\n",
    "# save as csv\n",
    "df.to_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape, df.Magnitude.min())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSV to 3D numpy array (where D3 are bins of magnitude) and save .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 week in this case)\n",
    "window_size = timedelta(weeks=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Define magnitude bin size and number of bins\n",
    "num_mag_bins = 10\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows, num_mag_bins), dtype=int)\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            \n",
    "            # Bin magnitudes between 0 and 10 and count number of earthquakes in each bin\n",
    "            magnitudes = bin_data['Magnitude']\n",
    "            counts, _ = np.histogram(magnitudes, bins=np.linspace(0, 10, num_mag_bins+1))\n",
    "\n",
    "            # Store counts in tensor\n",
    "            bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "            tensor[bin_idx, i, :] = counts\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "\n",
    "# Print tensor shape\n",
    "np.save('data/Japan_10_60_134_174_1973_2023.npy',tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CSV to 2D numpy array where rows are 2x2 degrees pixels and columns are bins of time of a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            # Check if there are any earthquakes in the current bin\n",
    "            if not bin_data.empty:\n",
    "                # Find the maximum magnitude in the current bin\n",
    "                max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "                # Store maximum magnitude in tensor\n",
    "                bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "                tensor[bin_idx, i] = max_mag\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 5\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Create DataFrame to store bin information\n",
    "bins_df = pd.DataFrame(columns=['Longitude', 'Latitude'])\n",
    "\n",
    "# Iterate over spatial bins\n",
    "for lon in range(min_lon, max_lon, bin_size):\n",
    "    for lat in range(min_lat, max_lat, bin_size):\n",
    "        # Add bin information to DataFrame\n",
    "        bins_df = pd.concat([bins_df, pd.DataFrame({'Longitude': lon, 'Latitude': lat}, index=[0])], ignore_index=True)\n",
    "\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for j in range(num_bins):\n",
    "        lon = bins_df.loc[j, 'Longitude']\n",
    "        lat = bins_df.loc[j, 'Latitude']\n",
    "        \n",
    "        # Get earthquake data within current spatial bin\n",
    "        bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                               (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "        # Check if there are any earthquakes in the current bin\n",
    "        if not bin_data.empty:\n",
    "            # Find the maximum magnitude in the current bin\n",
    "            max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "            # Store maximum magnitude in tensor\n",
    "            bin_idx = j\n",
    "            tensor[bin_idx, i] = max_mag\n",
    "\n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "# Add longitude and latitude columns to DataFrame\n",
    "bins_df['Longitude'] += bin_size/2\n",
    "bins_df['Latitude'] += bin_size/2\n",
    "\n",
    "print(bins_df.head())\n",
    "print(tensor.shape)\n",
    "df = pd.concat((pd.DataFrame(tensor), bins_df), axis=1)\n",
    "# df.to_csv('data/Japan_10_60_134_174_D_5X5_1973_2023.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as crs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').sort_values('Magnitude')\n",
    "# Create a map using Cartopy to display earthquake data with magnitudes, longitude, and latitude\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=crs.Mercator())\n",
    "ax.add_feature(cartopy.feature.LAND, facecolor=[.8,.8,.8])\n",
    "ax.add_feature(cartopy.feature.OCEAN, facecolor=[.95,.95,.95])\n",
    "ax.add_feature(cartopy.feature.COASTLINE,linewidth=0.3)\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle=':',linewidth=0.3)\n",
    "\n",
    "# Add gridlines\n",
    "lon = np.linspace(-180,180,181)\n",
    "lat = np.linspace(-90,90,91)\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "gl.xlocator = mticker.FixedLocator(lon)\n",
    "gl.ylocator = mticker.FixedLocator(lat)\n",
    "gl.top_labels = gl.right_labels = False\n",
    "gl.rotate_labels = True\n",
    "#gl.xlabel_style = {'rotation': 45}\n",
    "\n",
    "# Add coastlines\n",
    "ax.coastlines(color='black', linewidth=0.5)\n",
    "\n",
    "# Plot the earthquake data as scatter points\n",
    "sc = ax.scatter(df['Longitude'], df['Latitude'], c=df['Magnitude'], cmap=\"inferno\", s=np.exp(df['Magnitude'])/100, transform=crs.PlateCarree())\n",
    "\n",
    "# Set the colorbar and its label\n",
    "cbar = fig.colorbar(sc, ax=ax, fraction=0.04, pad=0.02)\n",
    "cbar.set_label('Magnitude')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Earthquakes between 1973 and 2023')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "# Set the bounds of the map to the minimum and maximum longitude and latitude values\n",
    "# Determine the minimum and maximum longitude and latitude values\n",
    "min_lon, max_lon = df['Longitude'].min(), df['Longitude'].max()\n",
    "min_lat, max_lat = df['Latitude'].min(), df['Latitude'].max()\n",
    "ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=crs.PlateCarree())\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plt.scatter(df.Time, df.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2D histogram of amount of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').iloc[:,4:]\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(4.8,6)})\n",
    "g = sns.histplot(\n",
    "    df, x=\"Longitude\", y=\"Latitude\",\n",
    "    bins=(20,25), cbar=True)\n",
    "\n",
    "g.set_xticks(ticks=np.linspace(134, 174, 21), labels=np.linspace(134, 174, 21).astype(int), rotation = 90)\n",
    "\n",
    "g.set_yticks(ticks=np.linspace(10, 60, 26), labels=np.linspace(10, 60, 26).astype(int))\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the most active 2x2 area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "pixel = df[(df.Longitude > 140) & (df.Longitude < 142) & (df.Latitude > 36) & (df.Latitude < 38)]\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.scatter(pixel.Time, pixel.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.jointplot(\n",
    "    data=df, x=\"Time\", y=\"Magnitude\", s=1, marginal_ticks=True, marginal_kws=dict(bins=74)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlation between pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_1973_2023.csv').iloc[:,:-2]\n",
    "df = df[(df > 0).any(axis=1)]\n",
    "df = df.rolling(14,axis=1, center=True, min_periods=0).mean()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.matshow(np.corrcoef(df),0, cmap='seismic',vmin=-1, vmax=1)\n",
    "plt.xlabel(\"2x2 grid pixel\")\n",
    "plt.ylabel(\"2x2 grid pixel\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ARIMA on 2x2 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# load earthquake data for a specific area\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data = data[(data.Longitude > 136) & (data.Longitude < 146) & (data.Latitude > 32) & (data.Latitude < 42)]\n",
    "data.set_index('Time', inplace=True)\n",
    "data = data['Magnitude'].resample('W').max()  # resample by day and get the maximum magnitude of the day\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# fit an ARIMA model to get the summary\n",
    "model = ARIMA(data, order=(3, 0, 3))  # (p, d, q) order\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\"\"\"\n",
    "\n",
    "train_size = 52*20\n",
    "total_splits = len(data)-train_size\n",
    "test_size = 1\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=total_splits, max_train_size=train_size ,test_size=test_size)\n",
    "\n",
    "mae_total = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "mag = 6\n",
    "\n",
    "for train_index, test_index in cv.split(data):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "    # fit an ARIMA model\n",
    "    model = ARIMA(data[train_index], order=(1, 1, 0))  # (p, d, q) order\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # forecast next week's magnitudes\n",
    "    forecast = model_fit.forecast(steps=test_size)\n",
    "    print('true:', data[test_index][0], 'prediction:', round(forecast[0],1))\n",
    "\n",
    "    # evaluate model performance\n",
    "    mae = mean_absolute_error(data[test_index], forecast)\n",
    "    mae_total += mae\n",
    "    #print('inermediate MSE:', mse)\n",
    "\n",
    "    if data[test_index][0] >= mag:\n",
    "        y_true.append(1)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            TP += 1\n",
    "        if forecast[0] < mag:\n",
    "            FN += 1\n",
    "            y_pred.append(0)\n",
    "    if data[test_index][0] < mag:\n",
    "        y_true.append(0)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            FP += 1\n",
    "        if forecast[0] < mag:\n",
    "            y_pred.append(0)\n",
    "            TN += 1\n",
    "\n",
    "acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "precision = TP / (TP+FP)\n",
    "recall = TP / (TP+FN)\n",
    "specificity = TN / (TN+FP)\n",
    "\n",
    "print('accuracy:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity:', specificity)\n",
    "print('Mean Absolute Error:', mae_total/total_splits)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = ['M<6','M>=6']\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualise ACF to define MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_val = acf(data)\n",
    "plt.bar(range(0,len(acf_val)),acf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize PACF to define AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_val = pacf(data)\n",
    "plt.bar(range(0,len(pacf_val)),pacf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise distribution of magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_5X5_1973_2023.csv').iloc[:,:-2]\n",
    "print(np.unique(df))\n",
    "print(\"mean:\", np.mean(np.array(df)[df > 0]))\n",
    "\n",
    "X = 6\n",
    "print(\"percentage of values bigger than X:\", (np.array(df) >= X).sum() / len(np.array(df).flatten()))\n",
    "sns.histplot(np.array(df).flatten(), bins=90, log_scale=(False,True))\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of earthquake magnitudes')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate one step problem with lstm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Flatten, Input, TimeDistributed, Dropout, RepeatVector\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# import data and cut off last two columns of longitude and latitude and transpose\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_5X5_1973_2023.csv').iloc[:,:-2].T\n",
    "\n",
    "### resample timeperiod original csv and get Time\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').iloc[:,4:]\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data.set_index('Time', inplace=True)\n",
    "data = data.sort_index()\n",
    "time_D = data.resample('D').max().fillna(0).index\n",
    "\n",
    "df = df.set_index(time_D)\n",
    "\n",
    "# resample time\n",
    "df = df.set_index(time_D).resample('W').max()\n",
    "\n",
    "# only contain columns where at least a mangitude bigger than zero is recorded\n",
    "dataset_df = df.T[(df > 0).any(axis=0)]\n",
    "\n",
    "\n",
    "# transpose dataset and convert to numpy array\n",
    "dataset = np.array(dataset_df.T)\n",
    "\n",
    "# define magnitude cutoff\n",
    "mag = 5\n",
    "mag_select = (dataset >= mag)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split data in train en test set\n",
    "train, test = train_test_split(dataset, test_size=.3, shuffle=False, random_state=43)\n",
    "mag_train, mag_test = train_test_split(mag_select, test_size=.3, shuffle=False, random_state=43)\n",
    "\n",
    "# define generator\n",
    "n_features = dataset.shape[1]\n",
    "n_input = 12\n",
    "steps_epoch = 120\n",
    "train_generator = TimeseriesGenerator(train, mag_train.astype(int), length=n_input, batch_size=(len(train) - n_input) // steps_epoch, shuffle=False)\n",
    "test_generator = TimeseriesGenerator(test, mag_test.astype(int), length=n_input, batch_size=(len(test) - n_input) // steps_epoch, shuffle=False)\n",
    "\n",
    "#########################\n",
    "\"\"\"\n",
    "input= Input(shape=(n_input, n_features))\n",
    "\n",
    "lstm1 = LSTM(12,return_state=True)\n",
    "LSTM_output, state_h, state_c = lstm1(input) \n",
    "states = [state_h, state_c]\n",
    "\n",
    "repeat=RepeatVector(1)\n",
    "LSTM_output = repeat(LSTM_output)\n",
    "\n",
    "lstm2 = LSTM(12,return_sequences=True)\n",
    "all_state_h = lstm2(LSTM_output,initial_state=states)\n",
    "\n",
    "\n",
    "dense = TimeDistributed(Dense(n_features, activation='sigmoid'))\n",
    "output = dense(all_state_h)\n",
    "model_LSTM_return_state = Model(input,output,name='model_LSTM_return_state')\n",
    "\n",
    "model_LSTM_return_state.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\"\"\"\n",
    "########################\n",
    "\n",
    "# E2D2\n",
    "#\n",
    "encoder_inputs = Input(shape=(n_input, n_features))\n",
    "encoder_l1 = LSTM(24,return_sequences = True, return_state=True, activation='tanh')\n",
    "encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "encoder_states1 = encoder_outputs1[1:]\n",
    "encoder_l2 = LSTM(24, return_state=True, activation='tanh')\n",
    "encoder_outputs2 = encoder_l2(encoder_outputs1[0])\n",
    "encoder_states2 = encoder_outputs2[1:]\n",
    "#\n",
    "decoder_inputs = RepeatVector(1)(encoder_outputs2[0])\n",
    "#\n",
    "decoder_l1 = LSTM(24, return_sequences=True, activation='tanh')(decoder_inputs,initial_state = encoder_states1)\n",
    "decoder_l2 = LSTM(24, return_sequences=True, activation='tanh')(decoder_l1,initial_state = encoder_states2)\n",
    "decoder_outputs2 = TimeDistributed(Dense(n_features, activation='sigmoid'))(decoder_l2)\n",
    "#\n",
    "model = Model(encoder_inputs,decoder_outputs2)\n",
    "#\n",
    "\n",
    "# Define custom loss function with class weights\n",
    "def weighted_loss(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, dtype='float32')\n",
    "        y_pred = K.cast(y_pred, dtype='float32')\n",
    "        loss_per_sample = weights[0] * y_true * K.log(y_pred) + weights[1] * (1 - y_true) * K.log(1 - y_pred)\n",
    "        return -K.mean(loss_per_sample, axis=-1)\n",
    "    return loss\n",
    "\n",
    "# Set class weights\n",
    "class_weights = {0: 0.001, 1: 0.999}\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "\n",
    "# early stopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\"\"\"\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(12, activation='tanh', input_shape=(n_input, n_features)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(230, activation='sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "\"\"\"\n",
    "\n",
    "# fit model\n",
    "history = model.fit(train_generator, validation_data=test_generator, steps_per_epoch=steps_epoch, epochs=1000, verbose=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "y_pred = scaler.inverse_transform(y_pred.reshape((y_pred.shape[0],y_pred.shape[2])))\n",
    "y_test = scaler.inverse_transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "class_names = ['M<6', 'M>=6']\n",
    "\n",
    "print(classification_report(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p = sns.heatmap(confusion_matrix(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5), annot=True, fmt='g')\n",
    "p.set_xlabel(\"Predicted\")\n",
    "p.set_ylabel(\"True\")\n",
    "p.xaxis.set_ticklabels(['M<6', 'M>6'], ha=\"center\", va=\"center\")\n",
    "p.yaxis.set_ticklabels(['M<6', 'M>6'], rotation=0, va=\"center\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_ROC_AUC(X,y):\n",
    "\n",
    "    # Use the trained model to predict the class probabilities for the validation set\n",
    "    y_prob = model.predict(X)\n",
    "    y_pred = scaler.inverse_transform(y_prob)\n",
    "    y_test = scaler.inverse_transform(y)\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_test[:-n_input].flatten() >= mag, y_pred.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
    "            ''.format(roc_auc), color='blue', linewidth=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Test)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig(savefig)\n",
    "    plt.show()\n",
    "\n",
    "plot_ROC_AUC(test_generator, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 5D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose frequency and binsize\n",
    "freq = 'W'\n",
    "binsize = 2\n",
    "\n",
    "# Load earthquake data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv').iloc[:,4:]\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "# Bin the longitude and latitude values into 2x2 degree bins\n",
    "df['Longitude_bin'] = pd.cut(df['Longitude'], bins=np.arange(134, 175, binsize))\n",
    "df['Latitude_bin'] = pd.cut(df['Latitude'], bins=np.arange(10, 61, binsize))\n",
    "\n",
    "# Group the data by longitude bin, latitude bin, and day, and compute the maximum magnitude within each group\n",
    "grouped = df.groupby(['Longitude_bin', 'Latitude_bin', pd.Grouper(freq=freq, level='Time')])['Magnitude'].max()\n",
    "\n",
    "# Convert the resulting data to a DataFrame, filling missing values with 0\n",
    "grouped_df = grouped.unstack().fillna(0)\n",
    "\n",
    "# Reshape the resulting data into a tensor with shape (1, time, rows, cols, channels)\n",
    "time = len(grouped_df.columns)\n",
    "rows = len(grouped_df.index.levels[0])\n",
    "cols = len(grouped_df.index.levels[1])\n",
    "tensor_convLSTM = np.zeros((1, time, rows, cols, 1))\n",
    "\n",
    "for t in range(time):\n",
    "    tensor_convLSTM[0, t, :, :, 0] = grouped_df.iloc[:, t].values.reshape(rows, cols)\n",
    "\n",
    "# Print the shape of the resulting tensor\n",
    "print(tensor_convLSTM.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot timesteps of 5D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose a timestep to plot (e.g. the first timestep)\n",
    "timestep = 1500\n",
    "\n",
    "# Extract the data for the chosen timestep from the tensor\n",
    "data = tensor_convLSTM[0, timestep, :, :, 0]\n",
    "\n",
    "# Create a boolean mask for values greater than 0\n",
    "mask = data > 0\n",
    "\n",
    "# Create a copy of the data array and set values less than or equal to 0 to NaN\n",
    "data_no_zeros = data.copy()\n",
    "data_no_zeros[~mask] = np.nan\n",
    "\n",
    "# Create a heatmap plot of the data using Seaborn\n",
    "sns.set(rc={'figure.figsize':(4.8,6)})\n",
    "sns.heatmap(data, cmap='viridis', vmin=-1, vmax=10, linewidths=0.5, linecolor='grey', annot=False)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(f'Earthquake magnitudes at timestep {timestep}')\n",
    "plt.xlabel('Longitude bin')\n",
    "plt.ylabel('Latitude bin')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data in train en test set\n",
    "dataset_convLSTM = tensor_convLSTM.reshape((tensor_convLSTM.shape[1], tensor_convLSTM.shape[2], tensor_convLSTM.shape[3], tensor_convLSTM.shape[4]))\n",
    "\n",
    "train, val_test = train_test_split(dataset_convLSTM, test_size=.4, shuffle=False, random_state=43)\n",
    "val, test = train_test_split(val_test, test_size=.5, shuffle=False, random_state=43)\n",
    "\n",
    "train = train.reshape((1, train.shape[0], train.shape[1], train.shape[2], train.shape[3]))\n",
    "val = val.reshape((1, val.shape[0], val.shape[1], val.shape[2], val.shape[3]))\n",
    "test = test.reshape((1, test.shape[0], test.shape[1], test.shape[2], test.shape[3]))\n",
    "\n",
    "# We'll define a helper function to shift the frames, where `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
    "def create_shifted_frames(data):\n",
    "    x = data[:, 0 : data.shape[1] - 1, :, :]\n",
    "    y = data[:, 1 : data.shape[1], :, :]\n",
    "    return x, y\n",
    "\n",
    "# Apply the processing function to the datasets.\n",
    "x_train, y_train = create_shifted_frames(train)\n",
    "x_val, y_val = create_shifted_frames(val)\n",
    "x_test, y_test = create_shifted_frames(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hereafter is mainly redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat = pd.read_excel(\"/Users/jurrienboogert/Downloads/emdat_public_2023_02_08_query_uid-jHccdA.xlsx\", header=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat[emdat['Disaster Type'] == 'Earthquake'][['Dis Mag Value']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file='/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/NOAA/earthquakes-2023-02-11_10-24-26_+0100.tsv'\n",
    "NOAA=pd.read_table(tsv_file,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag']\n",
    "\n",
    "plt.hist(NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'], bins=80)\n",
    "plt.show()\n",
    "\n",
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'].quantile(.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(NOAA[NOAA['Year'] > 2010][['year', 'month', 'day', 'hour', 'minute']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk1.csv\", low_memory=False)\n",
    "chunk2 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk2.csv\", low_memory=False)\n",
    "chunk3 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk3.csv\", low_memory=False)\n",
    "chunk4 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk4.csv\", low_memory=False)\n",
    "chunk5 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk5.csv\", low_memory=False)\n",
    "chunk6 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk6.csv\", low_memory=False)\n",
    "\n",
    "chunks = pd.concat([chunk2,chunk3,chunk4,chunk5,chunk6], ignore_index=True)\n",
    "chunks['trace_start_time'] = pd.to_datetime(chunks['trace_start_time'])\n",
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "chunks = chunks.sort_values('source_origin_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset = chunks[(chunks['source_longitude'] >= 19) & (chunks['source_longitude'] <= 30) & (chunks['source_latitude'] >= 34) & (chunks['source_latitude'] <= 44) & (chunks['source_origin_time'] <= '2015-6-25 03:14:47.900')]\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "plt.scatter(subset.source_origin_time, subset.source_magnitude, s=1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(subset.source_magnitude, bins=48)\n",
    "plt.xlabel('magnitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a histogram of the longitude values\n",
    "plt.hist(subset.source_latitude, bins=360)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Longitude')\n",
    "plt.show()\n",
    "\n",
    "# create a histogram of the latitude values\n",
    "plt.hist(subset.source_longitude, bins=360)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "\n",
    "# Load earthquake data\n",
    "df = subset[chunks['source_magnitude'] >= 0].sort_values('source_magnitude')\n",
    "\n",
    "# Extract latitude and longitude columns\n",
    "latitudes = df['source_latitude']\n",
    "longitudes = df['source_longitude']\n",
    "magnitudes = df['source_magnitude']\n",
    "\n",
    "# Set up map projection\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "map = Basemap(projection='merc', lat_0=0, lon_0=0, resolution='l',\n",
    "              llcrnrlon=19, llcrnrlat=33, urcrnrlon=30, urcrnrlat=43)\n",
    "\n",
    "# Draw coastlines, countries, and states\n",
    "#map.drawcoastlines(color='gray')\n",
    "map.fillcontinents(color='lightgray', lake_color='white')\n",
    "map.drawmapboundary(fill_color='white')\n",
    "\n",
    "# Draw parallels and meridians\n",
    "map.drawparallels(range(-90, 90, 1), linewidth=0.5, labels=[1, 0, 0, 0])\n",
    "meridians = map.drawmeridians(range(-180, 180, 1), linewidth=0.5, labels=[0, 0, 0, 1])\n",
    "\n",
    "for m in meridians:\n",
    "    try:\n",
    "        meridians[m][1][0].set_rotation(45)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Convert latitude and longitude to map coordinates\n",
    "x, y = map(longitudes, latitudes)\n",
    "\n",
    "# Plot earthquake magnitudes as circles on the map\n",
    "map.scatter(x, y, s=np.exp(magnitudes)/50, c=magnitudes, cmap='plasma', alpha=1)\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Earthquake Magnitudes')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# group the data by date and location to create the input data for Prophet\n",
    "data = chunks.groupby([pd.Grouper(key='source_origin_time', freq='D'), 'source_latitude', 'source_longitude']).size().reset_index(name='count')\n",
    "\n",
    "# rename columns for use with Prophet\n",
    "data = data.rename(columns={'source_origin_time': 'ds', 'count': 'y'})\n",
    "\n",
    "# create a Prophet model and fit the data\n",
    "model = Prophet()\n",
    "model.fit(data)\n",
    "\n",
    "# create a future dataframe with predictions for the next 365 days\n",
    "future = model.make_future_dataframe(periods=7)\n",
    "\n",
    "# predict the number of earthquakes for the future dates\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# plot the forecast\n",
    "fig = model.plot(forecast, xlabel='Date', ylabel='Number of Earthquakes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chunks['source_origin_time'], chunks['source_magnitude'], 'ro', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[['source_magnitude', 'trace_start_time', 'source_origin_time', 'receiver_latitude', 'receiver_longitude', 'source_latitude', 'source_longitude']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[(chunks['source_origin_time'].dt.year == 2007) & (chunks['source_origin_time'].dt.month == 8) & (chunks['source_origin_time'].dt.day == 17) & (chunks['source_origin_time'].dt.hour == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[chunks['source_magnitude'] > 6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Year'] > 1983) & (NOAA['Deaths'] > 0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"/Users/jurrienboogert/Downloads/2023.csv\", header=None, names=['ID', 'YEAR/MONTH/DAY', 'ELEMENT', 'DATA VALUE', 'M-FLAG', 'Q-FLAG', 'S-FLAG', 'OBS-TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dly = pd.read_fwf('/Users/jurrienboogert/Downloads/ghcnd_gsn/ghcnd_gsn/USW00013782.dly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = chunks.sample(1)[['source_latitude', 'source_longitude']]\n",
    "source.iloc[0,0], source.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "# Set time period\n",
    "start = datetime(1984, 9, 7, 2)\n",
    "end = datetime(1984, 9, 7, 3)\n",
    "\n",
    "# Create Point for Vancouver, BC\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 200000\n",
    "Point.max_count = 6\n",
    "Point.weight_dist = .6\n",
    "receiver = Point(source.iloc[0,0], source.iloc[0,1])\n",
    "# Get daily data for 2018\n",
    "data = Hourly(receiver, start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "year = chunks['source_origin_time'].dt.year.fillna(0).astype('int')\n",
    "month = chunks['source_origin_time'].dt.month.fillna(0).astype('int')\n",
    "day = chunks['source_origin_time'].dt.day.fillna(0).astype('int')\n",
    "hour = chunks['source_origin_time'].dt.hour.fillna(0).astype('int')\n",
    "\n",
    "chunks.source_latitude\n",
    "chunks.source_longitude\n",
    "\n",
    "chunks.receiver_latitude\n",
    "chunks.receiver_longitude\n",
    "chunks.receiver_elevation_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "temp_source = []\n",
    "rhum_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "rhum_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i], hour[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Hourly(source, start, end).fetch()['temp'][0])\n",
    "    rhum_source.append(Hourly(source, start, end).fetch()['rhum'][0])\n",
    "    pres_source.append(Hourly(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "temp_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Daily(source, start, end).fetch()['tavg'][0])\n",
    "    pres_source.append(Daily(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    Point.alt_range = chunks.receiver_elevation_m[i]\n",
    "    Point.adapt_temp = True\n",
    "    receiver = Point(chunks.receiver_latitude[i], chunks.receiver_longitude[i], chunks.receiver_elevation_m[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_receiver.append(Daily(receiver, start, end).fetch()['tavg'][0])\n",
    "    pres_receiver.append(Daily(receiver, start, end).fetch()['pres'][0])\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8acd2a4c40bb06440d03e583eeea35c6596324a3385dafe16353bbc1939be192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
