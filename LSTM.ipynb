{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18260, 10, 8, 4)\n",
      "(18260, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose frequency, binsize, longitude, latitude\n",
    "freq = 'D'\n",
    "binsize = 5\n",
    "longitude_W = 134  # minimum is 134\n",
    "longitude_E = 174  # maximum is 174\n",
    "latitude_S = 10  # minimum is 10\n",
    "latitude_N = 60  # minimum is 60\n",
    "\n",
    "# load earthquake data for defined area\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv', index_col=0)\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data = data[(data.Longitude >= longitude_W) & (data.Longitude <= longitude_E) & (data.Latitude >= latitude_S) & (data.Latitude <= latitude_N)]\n",
    "data.set_index('Time', inplace=True)\n",
    "df = data.sort_index()\n",
    "\n",
    "# Bin the longitude and latitude values into 2x2 degree bins\n",
    "df['Longitude_bin'] = pd.cut(df['Longitude'], bins=np.arange(longitude_W, longitude_E + 1, binsize))  # Change bin size to 2 degrees\n",
    "df['Latitude_bin'] = pd.cut(df['Latitude'], bins=np.arange(latitude_S, latitude_N + 1, binsize))  # Change bin size to 2 degrees\n",
    "\n",
    "# Group the data by longitude bin, latitude bin, depth bin, and day, and compute the maximum magnitude within each group\n",
    "grouped = df.groupby(['Longitude_bin', 'Latitude_bin', pd.Grouper(freq=freq, level=\"Time\")]).agg({'Magnitude': 'max', 'Depth': 'mean'})\n",
    "grouped = grouped.unstack().fillna(0)\n",
    "\n",
    "# Reshape the resulting data into a tensor with shape (time, longitude, latitude, 4)\n",
    "time = len(grouped.columns.levels[1])\n",
    "longitude = len(grouped.index.levels[0])\n",
    "latitude = len(grouped.index.levels[1])\n",
    "tensor = np.zeros((time, longitude, latitude, 4))\n",
    "\n",
    "for t in range(time):\n",
    "    tensor[t, :, :, 0] = grouped['Magnitude'].iloc[:, t].values.reshape(longitude, latitude)\n",
    "    tensor[t, :, :, 3] = grouped['Depth'].iloc[:, t].values.reshape(longitude, latitude)\n",
    "\n",
    "latitude_values = np.linspace(latitude_S, latitude_N - binsize, latitude) + binsize / 2\n",
    "longitude_values = np.linspace(longitude_W, longitude_E - binsize, longitude) + binsize / 2\n",
    "\n",
    "tensor[:, :, :, 1] = np.repeat(latitude_values, longitude).reshape(latitude, longitude).T\n",
    "tensor[:, :, :, 2] = np.tile(longitude_values, latitude).reshape(latitude, longitude).T\n",
    "\n",
    "# Rotate dimensions corresponding to 20 and 25, 90 degrees anti-clockwise\n",
    "tensor = np.transpose(tensor, axes=(0, 2, 1, 3))\n",
    "tensor = np.flip(tensor, axis=1)\n",
    "\n",
    "# Print the shape of the tensor\n",
    "print(tensor.shape)\n",
    "\n",
    "# Reshape tensor into matrix\n",
    "matrix = np.reshape(tensor[:,:,:,0], (tensor.shape[0], -1))\n",
    "\n",
    "print(matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = matrix.flatten() >= 0\n",
    "print((matrix.flatten()[select] >= 6).mean())\n",
    "alpha = 1 - (matrix.flatten()[select] >= 6).mean()\n",
    "print(alpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data in train, val, test\n",
    "\n",
    "train, val_test = train_test_split(matrix, test_size=.3, shuffle=False, random_state=43)\n",
    "val, test = train_test_split(val_test, test_size=.5, shuffle=False, random_state=43)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sliding window datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 180, 80) (420, 80)\n",
      "(85, 180, 80) (85, 80)\n",
      "(85, 180, 80) (85, 80)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import tensorflow as tf\n",
    "\n",
    "def dataset_generator(data, seq_length, samp_rate, stride, cutoff):\n",
    "\n",
    "  input_data = data\n",
    "  target_data = data\n",
    "  dataset = timeseries_dataset_from_array(input_data, targets=None, sequence_length=seq_length, sampling_rate=samp_rate, sequence_stride=stride, shuffle=False, batch_size=len(data))\n",
    "\n",
    "  return dataset\n",
    "\n",
    "# Set lookback timewindow\n",
    "lookback_window = 30 * 6\n",
    "cutoff = 4.5\n",
    "future_timesteps = 30\n",
    "timewindow = lookback_window + future_timesteps\n",
    "samp_rate = 1\n",
    "stride = future_timesteps\n",
    "\n",
    "train_dataset = dataset_generator(train, timewindow, samp_rate, stride, cutoff)\n",
    "val_dataset = dataset_generator(val, timewindow, samp_rate, stride, cutoff)\n",
    "test_dataset = dataset_generator(test, timewindow, samp_rate, stride, cutoff)\n",
    "\n",
    "# Create train set\n",
    "for batch in train_dataset:\n",
    "    X_train = batch[:,:-future_timesteps,:]\n",
    "    # X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "\n",
    "    y_train = batch[:,-future_timesteps:,:] >= cutoff\n",
    "    y_train = np.max(y_train, axis=1).astype(int)\n",
    "    # y_train = np.reshape(y_train, (y_train.shape[0], 1, y_train.shape[1])).astype(int)\n",
    "\n",
    "# y_train = tf.reshape(y_train, shape=[y_train.shape[0], 1, y_train.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_train = tf.cast(tf.reduce_max(y_train, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create validation set\n",
    "for batch in val_dataset:\n",
    "    X_val = batch[:,:-future_timesteps,:]\n",
    "    # X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "\n",
    "    y_val = batch[:,-future_timesteps:,:] >= cutoff\n",
    "    y_val = np.max(y_val, axis=1).astype(int)\n",
    "    # y_val = np.reshape(y_val, (y_val.shape[0], 1, y_val.shape[1])).astype(int)\n",
    "\n",
    "# y_val = tf.reshape(y_val, shape=[y_val.shape[0], 1, y_val.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_val = tf.cast(tf.reduce_max(y_val, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create test set\n",
    "for batch in test_dataset:\n",
    "    X_test = batch[:,:-future_timesteps,:]\n",
    "    # X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "    y_test = batch[:,-future_timesteps:,:] >= cutoff\n",
    "    y_test = np.max(y_test, axis=1).astype(int)\n",
    "    # y_test = np.reshape(y_test, (y_test.shape[0], 1, y_test.shape[1])).astype(int)\n",
    "\n",
    "# y_test = tf.reshape(y_test, shape=[y_test.shape[0], 1, y_test.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_test = tf.cast(tf.reduce_max(y_test, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Flatten, Input, TimeDistributed, Dropout, RepeatVector, BatchNormalization\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, ConvLSTM1D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64, activation='tanh',\n",
    "               return_sequences=True,\n",
    "               input_shape=(X_train.shape[1:]),\n",
    "               kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),\n",
    "               bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),\n",
    "               dropout=0.4,\n",
    "               ))\n",
    "               \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(64, activation='tanh', return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(y_train.shape[1], activation=\"sigmoid\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.BinaryFocalCrossentropy( apply_class_balancing=True, alpha=.95, gamma=5)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "model.build(input_shape=X_train.shape)\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "# fit model\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    batch_size=128,\n",
    "                    epochs=500,\n",
    "                    verbose=1,\n",
    "                    callbacks=[callback],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run 10 iterations of the model (and save it to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    keras.backend.clear_session()\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, activation='tanh',\n",
    "                return_sequences=True,\n",
    "                input_shape=(X_train.shape[1:]),\n",
    "                kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),\n",
    "                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),\n",
    "                dropout=0.4,\n",
    "                ))\n",
    "                \n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(64, activation='tanh', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y_train.shape[1], activation=\"sigmoid\"))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryFocalCrossentropy( apply_class_balancing=True, alpha=.95, gamma=5)\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    model.build(input_shape=X_train.shape)\n",
    "\n",
    "    # early stopping\n",
    "    callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        validation_data=(X_val,y_val),\n",
    "                        batch_size=128,\n",
    "                        epochs=500,\n",
    "                        verbose=1,\n",
    "                        callbacks=[callback],\n",
    "                        shuffle=False)\n",
    "    \n",
    "    model.save('models/LSTM_5x5_72M_1M_M6_iteration_{}'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('models/LSTM_5x5_72M_1M_M4.5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model = keras.models.load_model('models/LSTM_5x5_72M_1M_M4.5_iteration_1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "y_val = y_test\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(np.array(y_val).flatten(), y_pred.flatten() >= .5)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(np.array(y_val).flatten(), y_pred.flatten() >= .5)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(np.array(y_val).flatten(), y_pred.flatten() >= .5)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(np.array(y_val).flatten(), y_pred.flatten() >= .5)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "class_names = ['M<{}'.format(cutoff), 'M>={}'.format(cutoff)]\n",
    "\n",
    "print(classification_report(np.array(y_val).flatten(), y_pred.flatten() >= .5, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate mean(std) on the 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 25ms/step\n",
      "3/3 [==============================] - 1s 24ms/step\n",
      "3/3 [==============================] - 0s 23ms/step\n",
      "3/3 [==============================] - 0s 25ms/step\n",
      "3/3 [==============================] - 1s 24ms/step\n",
      "3/3 [==============================] - 0s 24ms/step\n",
      "3/3 [==============================] - 0s 24ms/step\n",
      "3/3 [==============================] - 0s 25ms/step\n",
      "3/3 [==============================] - 0s 24ms/step\n",
      "3/3 [==============================] - 0s 25ms/step\n",
      "Precision Mean: 0.758373 Std: 0.003853\n",
      "Recall Mean: 0.869268 Std: 0.002592\n",
      "F1 Mean: 0.810031 Std: 0.001370\n",
      "          P         R        F1\n",
      "0  0.751196  0.875261  0.808497\n",
      "1  0.757282  0.869686  0.809601\n",
      "2  0.761148  0.868293  0.811198\n",
      "3  0.754534  0.869686  0.808028\n",
      "4  0.761614  0.868293  0.811462\n",
      "5  0.757264  0.871777  0.810496\n",
      "6  0.762431  0.865505  0.810705\n",
      "7  0.764128  0.866899  0.812276\n",
      "8  0.754991  0.869686  0.808290\n",
      "9  0.759146  0.867596  0.809756\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "P = []\n",
    "R = []\n",
    "F1 = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    model = keras.models.load_model('models/LSTM_5x5_6M_1M_M4.5_iteration_{}'.format(i))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(np.array(y_test).flatten(), y_pred.flatten() >= .5)\n",
    "    P.append(precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(np.array(y_test).flatten(), y_pred.flatten() >= .5)\n",
    "    R.append(recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(np.array(y_test).flatten(), y_pred.flatten() >= .5)\n",
    "    F1.append(f1)\n",
    "\n",
    "    if i == 10:\n",
    "        df = pd.DataFrame((P,R,F1)).T.rename(columns={0: \"P\", 1: \"R\", 2: \"F1\"})\n",
    "        df.to_csv(\"LSTM_4.5_6M_iterations.csv\", index=False)\n",
    "        print('Precision Mean: %f' % np.mean(P), 'Std: %f' % np.std(P))\n",
    "        print('Recall Mean: %f' % np.mean(R), 'Std: %f' % np.std(R))\n",
    "        print('F1 Mean: %f' % np.mean(F1), 'Std: %f' % np.std(F1))\n",
    "        print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "p = sns.heatmap(confusion_matrix(np.array(y_val).flatten(), y_pred.flatten() >= 0.5), annot=True, fmt='g')\n",
    "p.set_xlabel(\"Predicted\")\n",
    "p.set_ylabel(\"True\")\n",
    "p.set_title(\"LSTM\")\n",
    "p.xaxis.set_ticklabels(['M<{}'.format(cutoff), 'M>={}'.format(cutoff)], ha=\"center\", va=\"center\")\n",
    "p.yaxis.set_ticklabels(['M<{}'.format(cutoff), 'M>={}'.format(cutoff)], rotation=0, va=\"center\")\n",
    "plt.savefig(\"graphs/CM_LSTM_5x5_1M_M45\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# y_pred = scaler.inverse_transform(y_prob)\n",
    "# y_test = scaler.inverse_transform(y)\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(np.array(y_val).flatten(), y_pred.flatten())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot micro-average ROC curve\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
    "        ''.format(roc_auc), color='blue', linewidth=1)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig(savefig)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(np.array(y_val).flatten(), y_pred.flatten())\n",
    "auc = auc(recall, precision)\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = (np.array(y_val).flatten()).sum() / len(np.array(y_val).flatten())\n",
    "plt.plot([0, 1], [no_skill,no_skill], linestyle='--', label='positive class = %.3f' % (no_skill))\n",
    "plt.plot(recall, precision, label='PR_curve, AUC = %.3f' % (auc))\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot predicted against true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape data to grid\n",
    "y_pred = np.reshape(y_pred, (y_pred.shape[0], tensor.shape[1], tensor.shape[2]))\n",
    "true = np.reshape(y_val, (y_val.shape[0], tensor.shape[1], tensor.shape[2]))\n",
    "\n",
    "# Choose timesteps to plot\n",
    "timestep = 8\n",
    "\n",
    "\n",
    "# Extract the data for the chosen timesteps from the tensor\n",
    "data1 = y_pred[timestep, :, :]\n",
    "data2 = true[timestep, :, :]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "# Plot the data in each subplot\n",
    "sns.heatmap(data1, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5, linecolor='grey', annot=False, ax=ax1)\n",
    "sns.heatmap(data2, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5, linecolor='grey', annot=False, ax=ax2)\n",
    "\n",
    "# Set the plot titles and axis labels\n",
    "ax1.set_title(f'Predicted M{cutoff} at timestep {timestep}')\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "\n",
    "# lon_centers = [(interval.left + interval.right) / 2 for interval in grouped.index.levels[0]]\n",
    "# lat_centers = [(interval.left + interval.right) / 2 for interval in grouped.index.levels[1]]\n",
    "ax1.set_xticklabels(grouped.index.levels[0], rotation=90)\n",
    "ax1.set_yticklabels(grouped.index.levels[1].sort_values(ascending=False), rotation=0)\n",
    "\n",
    "ax2.set_title(f'True M{cutoff} at timestep {timestep}')\n",
    "ax2.set_xticklabels(grouped.index.levels[0], rotation=90)\n",
    "ax2.set_yticklabels([])\n",
    "ax2.set_xlabel('Longitude')\n",
    "# ax2.set_ylabel(['Latitude'])\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"graphs/LSTM_72M_5x5_1M_M6_t8\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape data to grid\n",
    "y_pred = np.reshape(y_pred, (y_pred.shape[0], tensor.shape[1], tensor.shape[2]))\n",
    "true = np.reshape(y_val, (y_val.shape[0], tensor.shape[1], tensor.shape[2]))\n",
    "\n",
    "# Choose timesteps to plot\n",
    "timestep = 3\n",
    "\n",
    "\n",
    "# Extract the data for the chosen timesteps from the tensor\n",
    "data1 = y_pred[timestep, :, :]\n",
    "data2 = true[timestep, :, :]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "# Plot the data in each subplot\n",
    "sns.heatmap(data1, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5, linecolor='grey', annot=False, ax=ax1)\n",
    "sns.heatmap(data2, cmap='coolwarm', vmin=0, vmax=1, linewidths=0.5, linecolor='grey', annot=False, ax=ax2)\n",
    "\n",
    "# Set the plot titles and axis labels\n",
    "ax1.set_title(f'Predicted M{cutoff} at timestep {timestep}')\n",
    "ax2.set_title(f'True M{cutoff} at timestep {timestep}')\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax2.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "\n",
    "# Set the tick labels for the x and y axes of ax1\n",
    "ax1.set_xticks(np.arange(0.5, data1.shape[1], 1))\n",
    "ax1.set_xticklabels(grouped.index.levels[0][::1], rotation=90)\n",
    "ax1.set_yticks(np.arange(0.5, data1.shape[0], 1))\n",
    "ax1.set_yticklabels(grouped.index.levels[1][::-1].sort_values(ascending=False), rotation=0)\n",
    "\n",
    "# Set the tick labels for the x axis of ax2\n",
    "ax2.set_xticks(np.arange(0.5, data2.shape[1], 1))\n",
    "ax2.set_xticklabels(grouped.index.levels[0][::1], rotation=90)\n",
    "ax2.set_yticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"graphs/LSTM_72M_1M_5x5_M6_t4\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
