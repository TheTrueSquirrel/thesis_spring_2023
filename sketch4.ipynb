{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request data from USGS and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url = 'https://earthquake.usgs.gov/fdsnws/event/1/query'\n",
    "\n",
    "# Define the bounding box for the area of interest\n",
    "min_latitude = 10\n",
    "max_latitude = 60\n",
    "min_longitude = 134 #117 is wide\n",
    "max_longitude = 174 #165 more tight\n",
    "\n",
    "# Create an empty list to hold the earthquake data\n",
    "earthquakes = []\n",
    "\n",
    "for year in range(1973, 2023):\n",
    "    for month in range(1, 13):\n",
    "        # Set the parameters for the API request\n",
    "        starttime = f'{year}-{month:02d}-01'\n",
    "        endtime = f'{year}-{month+1:02d}-01'\n",
    "        if month == 12:\n",
    "            endtime = f'{year+1}-01-01'\n",
    "        params = {\n",
    "            'format': 'geojson',\n",
    "            'starttime': starttime,\n",
    "            'endtime': endtime,\n",
    "            'minmagnitude': '0',\n",
    "            'maxmagnitude': '10',\n",
    "            'minlatitude': min_latitude,\n",
    "            'maxlatitude': max_latitude,\n",
    "            'minlongitude': min_longitude,\n",
    "            'maxlongitude': max_longitude,\n",
    "            'mindepth': '0',\n",
    "            'maxdepth': '1000',\n",
    "            'eventtype': 'earthquake'\n",
    "        }\n",
    "\n",
    "        # Send the API request and get the response\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the data for each earthquake and append it to the list\n",
    "        for feature in data['features']:\n",
    "            longitude = feature['geometry']['coordinates'][0]\n",
    "            latitude = feature['geometry']['coordinates'][1]\n",
    "            time = pd.to_datetime(feature['properties']['time'], unit='ms')\n",
    "            magnitude = feature['properties']['mag']\n",
    "            depth = feature['geometry']['coordinates'][2]\n",
    "            earthquake = {'Longitude': longitude, 'Latitude': latitude, 'Time': time, 'Magnitude': magnitude, 'Depth': depth}\n",
    "            earthquakes.append(earthquake)\n",
    "\n",
    "# Create a DataFrame from the list of earthquake data\n",
    "df = pd.DataFrame(earthquakes)\n",
    "\n",
    "# Cut off magnitudes of 0\n",
    "df = df[df.Magnitude > 0]\n",
    "\n",
    "# save as csv\n",
    "df.to_csv('data/Japan_10_60_134_174_1973_2023_V2.csv')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df.shape, df.Magnitude.min())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSV to 3D numpy array (where D3 are bins of magnitude) and save .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 week in this case)\n",
    "window_size = timedelta(weeks=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Define magnitude bin size and number of bins\n",
    "num_mag_bins = 10\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows, num_mag_bins), dtype=int)\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            \n",
    "            # Bin magnitudes between 0 and 10 and count number of earthquakes in each bin\n",
    "            magnitudes = bin_data['Magnitude']\n",
    "            counts, _ = np.histogram(magnitudes, bins=np.linspace(0, 10, num_mag_bins+1))\n",
    "\n",
    "            # Store counts in tensor\n",
    "            bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "            tensor[bin_idx, i, :] = counts\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "\n",
    "# Print tensor shape\n",
    "np.save('data/Japan_10_60_134_174_1973_2023.npy',tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CSV to 2D numpy array where rows are 2x2 degrees pixels and columns are bins of time of a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 2\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for lon in range(min_lon, max_lon, bin_size):\n",
    "        for lat in range(min_lat, max_lat, bin_size):\n",
    "            # Get earthquake data within current spatial bin\n",
    "            bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                                   (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "            # Check if there are any earthquakes in the current bin\n",
    "            if not bin_data.empty:\n",
    "                # Find the maximum magnitude in the current bin\n",
    "                max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "                # Store maximum magnitude in tensor\n",
    "                bin_idx = (lon - min_lon)//bin_size*num_lat_bins + (lat - min_lat)//bin_size\n",
    "                tensor[bin_idx, i] = max_mag\n",
    "    \n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load earthquake data\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023.csv')\n",
    "\n",
    "# Define area of interest\n",
    "min_lon = 134\n",
    "max_lon = 174\n",
    "min_lat = 10\n",
    "max_lat = 60\n",
    "\n",
    "# Define time window size (1 day in this case)\n",
    "window_size = timedelta(days=1)\n",
    "\n",
    "# Calculate number of time windows\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "start_time = data['Time'].min()\n",
    "end_time = data['Time'].max()\n",
    "num_time_windows = (end_time - start_time) // window_size + 1\n",
    "\n",
    "# Define spatial bin size (2 degrees in this case)\n",
    "bin_size = 5\n",
    "\n",
    "# Calculate number of spatial bins\n",
    "num_lon_bins = int((max_lon - min_lon) / bin_size)\n",
    "num_lat_bins = int((max_lat - min_lat) / bin_size)\n",
    "num_bins = num_lon_bins * num_lat_bins\n",
    "\n",
    "# Create tensor with zeros\n",
    "tensor = np.zeros((num_bins, num_time_windows))\n",
    "\n",
    "# Create DataFrame to store bin information\n",
    "bins_df = pd.DataFrame(columns=['Longitude', 'Latitude'])\n",
    "\n",
    "# Iterate over spatial bins\n",
    "for lon in range(min_lon, max_lon, bin_size):\n",
    "    for lat in range(min_lat, max_lat, bin_size):\n",
    "        # Add bin information to DataFrame\n",
    "        bins_df = pd.concat([bins_df, pd.DataFrame({'Longitude': lon, 'Latitude': lat}, index=[0])], ignore_index=True)\n",
    "\n",
    "\n",
    "# Iterate over time windows\n",
    "for i in range(num_time_windows):\n",
    "    # Get earthquake data within current time window\n",
    "    mask = (data['Time'] >= start_time) & (data['Time'] < start_time + window_size)\n",
    "    window_data = data.loc[mask]\n",
    "\n",
    "    # Iterate over spatial bins\n",
    "    for j in range(num_bins):\n",
    "        lon = bins_df.loc[j, 'Longitude']\n",
    "        lat = bins_df.loc[j, 'Latitude']\n",
    "        \n",
    "        # Get earthquake data within current spatial bin\n",
    "        bin_data = window_data[(window_data['Longitude'] >= lon) & (window_data['Longitude'] < lon + bin_size) & \n",
    "                               (window_data['Latitude'] >= lat) & (window_data['Latitude'] < lat + bin_size)]\n",
    "\n",
    "        # Check if there are any earthquakes in the current bin\n",
    "        if not bin_data.empty:\n",
    "            # Find the maximum magnitude in the current bin\n",
    "            max_mag = bin_data['Magnitude'].max()\n",
    "\n",
    "            # Store maximum magnitude in tensor\n",
    "            bin_idx = j\n",
    "            tensor[bin_idx, i] = max_mag\n",
    "\n",
    "    # Increment time window\n",
    "    start_time += window_size\n",
    "\n",
    "# Add longitude and latitude columns to DataFrame\n",
    "bins_df['Longitude'] += bin_size/2\n",
    "bins_df['Latitude'] += bin_size/2\n",
    "\n",
    "print(bins_df.head())\n",
    "print(tensor.shape)\n",
    "df = pd.concat((pd.DataFrame(tensor), bins_df), axis=1)\n",
    "# df.to_csv('data/Japan_10_60_134_174_D_5X5_1973_2023.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as crs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv').sort_values('Magnitude')\n",
    "# Create a map using Cartopy to display earthquake data with magnitudes, longitude, and latitude\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=crs.Mercator())\n",
    "ax.add_feature(cartopy.feature.LAND, facecolor=[.8,.8,.8])\n",
    "ax.add_feature(cartopy.feature.OCEAN, facecolor=[.95,.95,.95])\n",
    "ax.add_feature(cartopy.feature.COASTLINE,linewidth=0.3)\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle=':',linewidth=0.3)\n",
    "\n",
    "# Add gridlines\n",
    "lon = np.linspace(-180,180,181)\n",
    "lat = np.linspace(-90,90,91)\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "gl.xlocator = mticker.FixedLocator(lon)\n",
    "gl.ylocator = mticker.FixedLocator(lat)\n",
    "gl.top_labels = gl.right_labels = False\n",
    "gl.rotate_labels = True\n",
    "#gl.xlabel_style = {'rotation': 45}\n",
    "\n",
    "# Add coastlines\n",
    "ax.coastlines(color='black', linewidth=0.5)\n",
    "\n",
    "# Plot the earthquake data as scatter points\n",
    "sc = ax.scatter(df['Longitude'], df['Latitude'], c=df['Magnitude'], cmap=\"inferno\", s=np.exp(df['Magnitude'])/100, transform=crs.PlateCarree())\n",
    "\n",
    "# Set the colorbar and its label\n",
    "cbar = fig.colorbar(sc, ax=ax, fraction=0.04, pad=0.02)\n",
    "cbar.set_label('Magnitude')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Earthquakes between 1973 and 2023')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "# Set the bounds of the map to the minimum and maximum longitude and latitude values\n",
    "# Determine the minimum and maximum longitude and latitude values\n",
    "min_lon, max_lon = df['Longitude'].min(), df['Longitude'].max()\n",
    "min_lat, max_lat = df['Latitude'].min(), df['Latitude'].max()\n",
    "ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=crs.PlateCarree())\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake magnitudes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plt.scatter(df.Time, df.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot earthquake depth over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plt.scatter(df.Time, df.Depth, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2D histogram of amount of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv')\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(4.8,6)})\n",
    "g = sns.histplot(\n",
    "    df, x=\"Longitude\", y=\"Latitude\",\n",
    "    bins=(20,25), cbar=True)\n",
    "\n",
    "g.set_xticks(ticks=np.linspace(134, 174, 21), labels=np.linspace(134, 174, 21).astype(int), rotation = 90)\n",
    "\n",
    "g.set_yticks(ticks=np.linspace(10, 60, 26), labels=np.linspace(10, 60, 26).astype(int))\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the most active 2x2 area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv')\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "pixel = df[(df.Longitude > 140) & (df.Longitude < 142) & (df.Latitude > 36) & (df.Latitude < 38)]\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.scatter(pixel.Time, pixel.Magnitude, s=.1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.jointplot(\n",
    "    data=df, x=\"Magnitude\", y=\"Depth\", s=1, marginal_ticks=True, marginal_kws=dict(bins=74)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot correlation between pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_1973_2023.csv').iloc[:,:-2]\n",
    "df = df[(df > 0).any(axis=1)]\n",
    "df = df.rolling(14,axis=1, center=True, min_periods=0).mean()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.matshow(np.corrcoef(df),0, cmap='seismic',vmin=-1, vmax=1)\n",
    "plt.xlabel(\"2x2 grid pixel\")\n",
    "plt.ylabel(\"2x2 grid pixel\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ARIMA on 10x10 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# load earthquake data for a specific area\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv')\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data = data[(data.Longitude > 140) & (data.Longitude < 145) & (data.Latitude > 35) & (data.Latitude < 40)]\n",
    "data.set_index('Time', inplace=True)\n",
    "data = data['Magnitude'].resample('M').max()  # resample by day and get the maximum magnitude of the day\n",
    "data = data.fillna(0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# fit an ARIMA model to get the summary\n",
    "model = ARIMA(data, order=(3, 0, 3))  # (p, d, q) order\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\"\"\"\n",
    "\n",
    "train_size = 12 * 2\n",
    "total_splits = len(data)-train_size\n",
    "test_size = 1\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=total_splits, max_train_size=train_size ,test_size=test_size)\n",
    "\n",
    "mae_total = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "mag = 6\n",
    "\n",
    "for train_index, test_index in cv.split(data):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "    # fit an ARIMA model\n",
    "    model = ARIMA(data[train_index], order=(1, 1, 0))  # (p, d, q) order\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # forecast next week's magnitudes\n",
    "    forecast = model_fit.forecast(steps=test_size)\n",
    "    # print('true:', data[test_index][0], 'prediction:', round(forecast[0],1))\n",
    "\n",
    "    # evaluate model performance\n",
    "    mae = mean_absolute_error(data[test_index], forecast)\n",
    "    mae_total += mae\n",
    "    #print('inermediate MSE:', mse)\n",
    "\n",
    "    if data[test_index][0] >= mag:\n",
    "        y_true.append(1)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            TP += 1\n",
    "        if forecast[0] < mag:\n",
    "            FN += 1\n",
    "            y_pred.append(0)\n",
    "    if data[test_index][0] < mag:\n",
    "        y_true.append(0)\n",
    "        if forecast[0] >= mag:\n",
    "            y_pred.append(1)\n",
    "            FP += 1\n",
    "        if forecast[0] < mag:\n",
    "            y_pred.append(0)\n",
    "            TN += 1\n",
    "\n",
    "acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "precision = TP / (TP+FP)\n",
    "recall = TP / (TP+FN)\n",
    "specificity = TN / (TN+FP)\n",
    "\n",
    "print('accuracy:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity:', specificity)\n",
    "print('Mean Absolute Error:', mae_total/total_splits)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "class_names = ['M<6','M>=6']\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualise ACF to define MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_val = acf(data)\n",
    "plt.bar(range(0,len(acf_val)),acf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize PACF to define AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf_val = pacf(data)\n",
    "plt.bar(range(0,len(pacf_val)),pacf_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise distribution of magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_D_5X5_1973_2023.csv').iloc[:,:-2]\n",
    "print(np.unique(df))\n",
    "print(\"mean:\", np.mean(np.array(df)[df > 0]))\n",
    "\n",
    "X = 6\n",
    "print(\"percentage of values bigger than X:\", (np.array(df) >= X).sum() / len(np.array(df).flatten()))\n",
    "sns.histplot(np.array(df).flatten(), bins=90, log_scale=(False,True))\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of earthquake magnitudes')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose frequency and binsize\n",
    "freq = 'M'\n",
    "binsize = 5\n",
    "\n",
    "# Load earthquake data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv')\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "# Bin the longitude and latitude values into 2x2 degree bins\n",
    "df['Longitude_bin'] = pd.cut(df['Longitude'], bins=np.arange(134, 175, binsize))\n",
    "df['Latitude_bin'] = pd.cut(df['Latitude'], bins=np.arange(10, 61, binsize))\n",
    "\n",
    "# Group the data by longitude bin, latitude bin, and day, and compute the maximum magnitude within each group\n",
    "grouped = df.groupby(['Longitude_bin', 'Latitude_bin', pd.Grouper(freq=freq, level='Time')])['Magnitude'].max()\n",
    "\n",
    "# Convert the resulting data to a DataFrame, filling missing values with 0\n",
    "grouped_df = grouped.unstack().fillna(0)\n",
    "\n",
    "# Reshape the resulting data into a tensor with shape (1, time, rows, cols, channels)\n",
    "time = len(grouped_df.columns)\n",
    "rows = len(grouped_df.index.levels[0])\n",
    "cols = len(grouped_df.index.levels[1])\n",
    "dataset = np.zeros((1, time, rows, cols, 1))\n",
    "\n",
    "for t in range(time):\n",
    "    dataset[0, t, :, :, 0] = grouped_df.iloc[:, t].values.reshape(rows, cols)\n",
    "\n",
    "# Rotate dimensions corresponding to 20 and 25, 90 degrees anti-clockwise\n",
    "dataset = np.transpose(dataset, axes=(0, 1, 3, 2, 4))\n",
    "dataset = np.flip(dataset, axis=2)\n",
    "\n",
    "dataset = dataset.reshape((dataset.shape[1], -1))\n",
    "dataset = dataset[:, dataset.any(axis=0)]\n",
    "# Print the shape of the resulting tensor\n",
    "print(dataset.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# define magnitude cutoff\n",
    "mag = 4.5\n",
    "mag_select = (dataset >= mag)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split data in train en test set\n",
    "train, test = train_test_split(dataset, test_size=.3, shuffle=False, random_state=43)\n",
    "mag_train, mag_test = train_test_split(mag_select, test_size=.3, shuffle=False, random_state=43)\n",
    "\n",
    "# define generator\n",
    "n_features = dataset.shape[1]\n",
    "n_input = 10\n",
    "steps_epoch = 32\n",
    "train_generator = TimeseriesGenerator(train, mag_train.astype(int), length=n_input, batch_size=(len(train) - n_input) // steps_epoch, shuffle=False)\n",
    "test_generator = TimeseriesGenerator(test, mag_test.astype(int), length=n_input, batch_size=(len(test) - n_input) // steps_epoch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate one step problem with lstm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Flatten, Input, TimeDistributed, Dropout, RepeatVector, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "#########################\n",
    "\"\"\"\n",
    "input= Input(shape=(n_input, n_features))\n",
    "\n",
    "lstm1 = LSTM(12,return_state=True)\n",
    "LSTM_output, state_h, state_c = lstm1(input) \n",
    "states = [state_h, state_c]\n",
    "\n",
    "repeat=RepeatVector(1)\n",
    "LSTM_output = repeat(LSTM_output)\n",
    "\n",
    "lstm2 = LSTM(12,return_sequences=True)\n",
    "all_state_h = lstm2(LSTM_output,initial_state=states)\n",
    "\n",
    "\n",
    "dense = TimeDistributed(Dense(n_features, activation='sigmoid'))\n",
    "output = dense(all_state_h)\n",
    "model_LSTM_return_state = Model(input,output,name='model_LSTM_return_state')\n",
    "\n",
    "model_LSTM_return_state.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "########################\n",
    "#\n",
    "encoder_inputs = Input(shape=(n_input, n_features))\n",
    "encoder_outputs1 = LSTM(12,return_sequences = True, return_state=True, activation='tanh')(encoder_inputs)\n",
    "encoder_states1 = encoder_outputs1[1:]\n",
    "encoder_outputs2 = LSTM(12, return_state=True, activation='tanh')(encoder_outputs1[0])\n",
    "\n",
    "encoder_states2 = encoder_outputs2[1:]\n",
    "#\n",
    "decoder_inputs = RepeatVector(1)(encoder_outputs2[0])\n",
    "#\n",
    "decoder_l1 = LSTM(12, return_sequences=True, activation='tanh')(decoder_inputs,initial_state = encoder_states1)\n",
    "decoder_l2 = LSTM(12, return_sequences=True, activation='tanh')(decoder_l1,initial_state = encoder_states2)\n",
    "decoder_outputs2 = TimeDistributed(Dense(n_features, activation='sigmoid'))(decoder_l2)\n",
    "#\n",
    "model = Model(encoder_inputs,decoder_outputs2)\n",
    "#\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "print(model.summary())\n",
    "\"\"\"\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(12, activation='tanh', return_sequences=True, input_shape=(n_input, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(12, activation='tanh', return_sequences=True, input_shape=(n_input, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(12, activation='tanh', input_shape=(n_input, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(n_features, activation='sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[keras.metrics.Precision(), keras.metrics.Recall(), 'accuracy'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# fit model\n",
    "history = model.fit(train_generator, validation_data=test_generator, steps_per_epoch=steps_epoch, epochs=1000, verbose=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "y_test = scaler.inverse_transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "class_names = ['M<6', 'M>=6']\n",
    "\n",
    "print(classification_report(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p = sns.heatmap(confusion_matrix(y_test[:-n_input].flatten() >= mag, y_pred.flatten() >= .5), annot=True, fmt='g')\n",
    "p.set_xlabel(\"Predicted\")\n",
    "p.set_ylabel(\"True\")\n",
    "p.xaxis.set_ticklabels(['M<6', 'M>6'], ha=\"center\", va=\"center\")\n",
    "p.yaxis.set_ticklabels(['M<6', 'M>6'], rotation=0, va=\"center\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_ROC_AUC(X,y):\n",
    "\n",
    "    # Use the trained model to predict the class probabilities for the validation set\n",
    "    y_prob = model.predict(X)\n",
    "    y_pred = scaler.inverse_transform(y_prob)\n",
    "    y_test = scaler.inverse_transform(y)\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_test[:-n_input].flatten() >= mag, y_pred.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
    "            ''.format(roc_auc), color='blue', linewidth=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Test)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig(savefig)\n",
    "    plt.show()\n",
    "\n",
    "plot_ROC_AUC(test_generator, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 5D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose frequency and binsize\n",
    "freq = 'M'  # Change frequency to days\n",
    "binsize = 5\n",
    "depthsize = 10\n",
    "# Load earthquake data into a pandas DataFrame\n",
    "df = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv')\n",
    "df['Time'] = pd.to_datetime(df.Time)\n",
    "df = df.set_index('Time')\n",
    "df = df.sort_index()\n",
    "\n",
    "# Bin the longitude and latitude values into 2x2 degree bins\n",
    "df['Longitude_bin'] = pd.cut(df['Longitude'], bins=np.arange(134, 175, binsize))  # Change bin size to 2 degrees\n",
    "df['Latitude_bin'] = pd.cut(df['Latitude'], bins=np.arange(10, 61, binsize))  # Change bin size to 2 degrees\n",
    "df['Depth_bin'] = pd.cut(df['Depth'], bins=np.arange(0, 700+depthsize, depthsize))\n",
    "\n",
    "# Group the data by longitude bin, latitude bin, depth bin, and day, and compute the maximum magnitude within each group\n",
    "grouped = df.groupby(['Longitude_bin', 'Latitude_bin', 'Depth_bin', pd.Grouper(freq=freq, level=\"Time\")]).max()['Magnitude']\n",
    "grouped = grouped.unstack().fillna(0)\n",
    "\n",
    "# Reshape the resulting data into a tensor_convLSTM with shape (1, time, depth, longitude, latitude)\n",
    "time = len(grouped.columns)\n",
    "depth = len(grouped.index.levels[2])\n",
    "longitude = len(grouped.index.levels[0])\n",
    "latitude = len(grouped.index.levels[1])\n",
    "tensor_convLSTM = np.zeros((1, time, longitude, latitude, depth))\n",
    "\n",
    "for t in range(time):\n",
    "    tensor_convLSTM[0, t, :, :, :] = grouped.iloc[:, t].values.reshape(longitude, latitude, depth)\n",
    "\n",
    "# Rotate dimensions corresponding to 20 and 25, 90 degrees anti-clockwise\n",
    "tensor_convLSTM = np.transpose(tensor_convLSTM, axes=(0, 1, 4, 3, 2))\n",
    "tensor_convLSTM = np.flip(tensor_convLSTM, axis=3)\n",
    "# Print the shape of the resulting tensor_convLSTM\n",
    "print(tensor_convLSTM.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot timesteps of 5D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose a timestep to plot (e.g. the first timestep)\n",
    "timestep = 40\n",
    "depth = 5\n",
    "\n",
    "# Extract the data for the chosen timestep from the tensor\n",
    "# tensor_convLSTM = tf.cast(tf.reduce_max(tensor_convLSTM, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "data = tensor_convLSTM[0, timestep, depth, :, :]\n",
    "\n",
    "# Create a heatmap plot of the data using Seaborn\n",
    "sns.set(rc={'figure.figsize':(4.8,6)})\n",
    "sns.heatmap(data, cmap='viridis', vmin=-1, vmax=10, linewidths=0.5, linecolor='grey', annot=False)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title(f'Earthquake magnitudes at timestep {timestep}')\n",
    "plt.xlabel('Longitude bin')\n",
    "plt.ylabel('Latitude bin')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data in train en test set\n",
    "dataset_convLSTM = tensor_convLSTM.reshape((tensor_convLSTM.shape[1], tensor_convLSTM.shape[2], tensor_convLSTM.shape[3], tensor_convLSTM.shape[4]))\n",
    "\n",
    "train, val_test = train_test_split(dataset_convLSTM, test_size=.4, shuffle=False, random_state=43)\n",
    "val, test = train_test_split(val_test, test_size=.5, shuffle=False, random_state=43)\n",
    "\n",
    "\"\"\"\n",
    "train = train.reshape((1, train.shape[0], train.shape[1], train.shape[2], train.shape[3]))\n",
    "val = val.reshape((1, val.shape[0], val.shape[1], val.shape[2], val.shape[3]))\n",
    "test = test.reshape((1, test.shape[0], test.shape[1], test.shape[2], test.shape[3]))\n",
    "\n",
    "# We'll define a helper function to shift the frames, where `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
    "def create_shifted_frames(data):\n",
    "    x = data[:, 0 : data.shape[1] - 1, :, :]\n",
    "    y = data[:, 1 : data.shape[1], :, :]\n",
    "    return x, y\n",
    "\n",
    "# Apply the processing function to the datasets.\n",
    "x_train, y_train = create_shifted_frames(train)\n",
    "x_val, y_val = create_shifted_frames(val)\n",
    "x_test, y_test = create_shifted_frames(test)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets from timeseries V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import tensorflow as tf\n",
    "\n",
    "def dataset_generator(data, seq_length, cutoff):\n",
    "\n",
    "  input_data = data # data[:-seq_length]\n",
    "  targets = data[seq_length:]\n",
    "  dataset = timeseries_dataset_from_array(input_data, (targets >= cutoff).astype(int), sequence_length=seq_length, sampling_rate=1, sequence_stride=1, shuffle=False, batch_size=len(data))\n",
    "  \"\"\"\n",
    "  for batch in dataset:\n",
    "    inputs, targets = batch\n",
    "    assert np.array_equal(inputs[0], data[:seq_length])  # First sequence: steps [0-9]\n",
    "    assert np.array_equal(targets[0], data[seq_length])  # Corresponding target: step 10\n",
    "    \"\"\"\n",
    "  return dataset\n",
    "\n",
    "# Set lookback timewindow\n",
    "timewindow = 10\n",
    "\n",
    "train_dataset = dataset_generator(train, timewindow, 4.5)\n",
    "val_dataset = dataset_generator(val, timewindow, 4.5)\n",
    "test_dataset = dataset_generator(test, timewindow, 4.5)\n",
    "\n",
    "# Create train set\n",
    "for batch in train_dataset:\n",
    "    X_train, y_train = batch\n",
    "\n",
    "y_train = tf.reshape(y_train, shape=[y_train.shape[0], 1, y_train.shape[1], y_train.shape[2], y_train.shape[3]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "y_train = tf.cast(tf.reduce_max(y_train, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create validation set\n",
    "for batch in val_dataset:\n",
    "    X_val, y_val = batch\n",
    "\n",
    "y_val = tf.reshape(y_val, shape=[y_val.shape[0], 1, y_val.shape[1], y_val.shape[2], y_val.shape[3]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "y_val = tf.cast(tf.reduce_max(y_val, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create test set\n",
    "for batch in test_dataset:\n",
    "    X_test, y_test = batch\n",
    "\n",
    "y_test = tf.reshape(y_test, shape=[y_test.shape[0], 1, y_test.shape[1], y_test.shape[2], y_test.shape[3]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "y_test = tf.cast(tf.reduce_max(y_test, axis=2, keepdims=True) > 0, dtype=tf.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets from timeseries V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import tensorflow as tf\n",
    "\n",
    "def dataset_generator(data, seq_length, cutoff):\n",
    "\n",
    "  input_data = data # data[:-seq_length]\n",
    "  targets = data[seq_length:]\n",
    "  dataset = timeseries_dataset_from_array(input_data, (targets >= cutoff).astype(int), sequence_length=seq_length, sampling_rate=1, sequence_stride=1, shuffle=False, batch_size=len(data))\n",
    "  \"\"\"\n",
    "  for batch in dataset:\n",
    "    inputs, targets = batch\n",
    "    assert np.array_equal(inputs[0], data[:seq_length])  # First sequence: steps [0-9]\n",
    "    assert np.array_equal(targets[0], data[seq_length])  # Corresponding target: step 10\n",
    "    \"\"\"\n",
    "  return dataset\n",
    "\n",
    "batches = dataset_generator(dataset_convLSTM, 10, 4.5)\n",
    "for batch in batches:\n",
    "    X, y = batch\n",
    "    \n",
    "y = tf.reshape(y, shape=[y.shape[0], 1, y.shape[1], y.shape[2], y.shape[3]])\n",
    "y = tf.cast(tf.reduce_max(y, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "X_train, X_val_test = train_test_split(np.array(X), test_size=.4, shuffle=True, random_state=43)\n",
    "X_val, X_test = train_test_split(X_val_test, test_size=.5, shuffle=True, random_state=43)\n",
    "\n",
    "y_train, y_val_test = train_test_split(np.array(y), test_size=.4, shuffle=True, random_state=43)\n",
    "y_val, y_test = train_test_split(y_val_test, test_size=.5, shuffle=True, random_state=43)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelconstruction of convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10, 70, 10, 8)]   0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 70, 10, 8)    32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 10, 12, 10, 8)     35472     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10, 12, 10, 8)    32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 10, 12, 10, 8)     4656      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 10, 12, 10, 8)    32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 1, 10, 8)          56        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1, 10, 8)         32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 1, 1, 10, 8)       0         \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, 1, 1, 10, 8)       1736      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,048\n",
      "Trainable params: 41,984\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, regularizers\n",
    "import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Construct the inputut layer with no definite frame size.\n",
    "input = layers.Input(shape=(X_train.shape[1:]))\n",
    "\n",
    "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
    "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
    "x = layers.BatchNormalization()(input)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=12,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    "    data_format = \"channels_first\",\n",
    "    kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=12,\n",
    "    kernel_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    "    data_format = \"channels_first\",\n",
    "    kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=1,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=False,\n",
    "    activation=\"relu\",\n",
    "    data_format = \"channels_first\",\n",
    "    kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = tf.reshape(x, (-1, 1, x.shape[1], x.shape[2], x.shape[3]))\n",
    "x = layers.Conv3D(filters=x.shape[4], kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "# Next, we will build the complete model and compile it.\n",
    "model = keras.models.Model(input, x)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[keras.metrics.Precision(), keras.metrics.Recall(), 'accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeltraining of convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 5s 185ms/step - loss: 0.6866 - precision: 0.3049 - recall: 0.7054 - accuracy: 0.3506 - val_loss: 0.7142 - val_precision: 0.2476 - val_recall: 0.5968 - val_accuracy: 0.2034 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.6447 - precision: 0.4086 - recall: 0.8539 - accuracy: 0.4986 - val_loss: 0.7053 - val_precision: 0.3358 - val_recall: 0.6009 - val_accuracy: 0.2856 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 0.6149 - precision: 0.4531 - recall: 0.9164 - accuracy: 0.5398 - val_loss: 0.6913 - val_precision: 0.4337 - val_recall: 0.6641 - val_accuracy: 0.3398 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 0.5891 - precision: 0.4819 - recall: 0.9411 - accuracy: 0.5780 - val_loss: 0.6738 - val_precision: 0.5030 - val_recall: 0.7053 - val_accuracy: 0.4110 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.5641 - precision: 0.5217 - recall: 0.9402 - accuracy: 0.5576 - val_loss: 0.6560 - val_precision: 0.5389 - val_recall: 0.7413 - val_accuracy: 0.4644 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 0.5390 - precision: 0.5510 - recall: 0.9422 - accuracy: 0.5407 - val_loss: 0.6379 - val_precision: 0.5770 - val_recall: 0.7684 - val_accuracy: 0.5025 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 0.5143 - precision: 0.5822 - recall: 0.9393 - accuracy: 0.5356 - val_loss: 0.6160 - val_precision: 0.6199 - val_recall: 0.7564 - val_accuracy: 0.5568 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 0.4901 - precision: 0.6136 - recall: 0.9293 - accuracy: 0.5328 - val_loss: 0.5959 - val_precision: 0.6627 - val_recall: 0.7788 - val_accuracy: 0.5669 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 0.4675 - precision: 0.6445 - recall: 0.9229 - accuracy: 0.5288 - val_loss: 0.5662 - val_precision: 0.6941 - val_recall: 0.7183 - val_accuracy: 0.5661 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 0.4428 - precision: 0.6816 - recall: 0.9146 - accuracy: 0.5302 - val_loss: 0.5475 - val_precision: 0.6976 - val_recall: 0.7256 - val_accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 0.4198 - precision: 0.7004 - recall: 0.9082 - accuracy: 0.5218 - val_loss: 0.5180 - val_precision: 0.7610 - val_recall: 0.6927 - val_accuracy: 0.5915 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.3971 - precision: 0.7087 - recall: 0.8948 - accuracy: 0.5073 - val_loss: 0.4940 - val_precision: 0.7169 - val_recall: 0.7027 - val_accuracy: 0.5856 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 0.3743 - precision: 0.7202 - recall: 0.8976 - accuracy: 0.5209 - val_loss: 0.4747 - val_precision: 0.7842 - val_recall: 0.6729 - val_accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.3536 - precision: 0.7243 - recall: 0.8856 - accuracy: 0.4791 - val_loss: 0.4459 - val_precision: 0.8043 - val_recall: 0.6755 - val_accuracy: 0.5627 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 0.3338 - precision: 0.7344 - recall: 0.8819 - accuracy: 0.5088 - val_loss: 0.4214 - val_precision: 0.8263 - val_recall: 0.6750 - val_accuracy: 0.5619 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 0.3152 - precision: 0.7523 - recall: 0.8759 - accuracy: 0.4847 - val_loss: 0.3969 - val_precision: 0.8341 - val_recall: 0.6661 - val_accuracy: 0.5585 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 0.2977 - precision: 0.7725 - recall: 0.8654 - accuracy: 0.4864 - val_loss: 0.3772 - val_precision: 0.8370 - val_recall: 0.6672 - val_accuracy: 0.5585 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 0.2823 - precision: 0.7843 - recall: 0.8635 - accuracy: 0.4819 - val_loss: 0.3679 - val_precision: 0.8190 - val_recall: 0.7011 - val_accuracy: 0.5458 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 0.2696 - precision: 0.7934 - recall: 0.8545 - accuracy: 0.4816 - val_loss: 0.3500 - val_precision: 0.8336 - val_recall: 0.6927 - val_accuracy: 0.5424 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.2579 - precision: 0.8043 - recall: 0.8481 - accuracy: 0.4879 - val_loss: 0.3318 - val_precision: 0.8380 - val_recall: 0.6964 - val_accuracy: 0.5525 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 0.2464 - precision: 0.8090 - recall: 0.8576 - accuracy: 0.4901 - val_loss: 0.3181 - val_precision: 0.8303 - val_recall: 0.7068 - val_accuracy: 0.5407 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 0.2379 - precision: 0.8177 - recall: 0.8472 - accuracy: 0.4893 - val_loss: 0.2995 - val_precision: 0.8341 - val_recall: 0.7001 - val_accuracy: 0.5508 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 0.2290 - precision: 0.8201 - recall: 0.8481 - accuracy: 0.4887 - val_loss: 0.2931 - val_precision: 0.8414 - val_recall: 0.6974 - val_accuracy: 0.5347 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 0.2247 - precision: 0.8253 - recall: 0.8295 - accuracy: 0.5008 - val_loss: 0.2710 - val_precision: 0.8431 - val_recall: 0.6781 - val_accuracy: 0.5508 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.2165 - precision: 0.8342 - recall: 0.8357 - accuracy: 0.4938 - val_loss: 0.2670 - val_precision: 0.8242 - val_recall: 0.7188 - val_accuracy: 0.5551 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.2091 - precision: 0.8432 - recall: 0.8378 - accuracy: 0.4980 - val_loss: 0.2565 - val_precision: 0.8215 - val_recall: 0.7204 - val_accuracy: 0.5237 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 0.2023 - precision: 0.8426 - recall: 0.8483 - accuracy: 0.4949 - val_loss: 0.2544 - val_precision: 0.8376 - val_recall: 0.7105 - val_accuracy: 0.5186 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.1979 - precision: 0.8446 - recall: 0.8477 - accuracy: 0.5102 - val_loss: 0.2487 - val_precision: 0.8179 - val_recall: 0.7449 - val_accuracy: 0.5220 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.1919 - precision: 0.8502 - recall: 0.8592 - accuracy: 0.4994 - val_loss: 0.2450 - val_precision: 0.8069 - val_recall: 0.7522 - val_accuracy: 0.5093 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 3s 212ms/step - loss: 0.1870 - precision: 0.8526 - recall: 0.8589 - accuracy: 0.4977 - val_loss: 0.2508 - val_precision: 0.8162 - val_recall: 0.7251 - val_accuracy: 0.5339 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 0.1862 - precision: 0.8570 - recall: 0.8414 - accuracy: 0.5107 - val_loss: 0.2410 - val_precision: 0.8056 - val_recall: 0.7522 - val_accuracy: 0.5178 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 0.1810 - precision: 0.8562 - recall: 0.8538 - accuracy: 0.5102 - val_loss: 0.2324 - val_precision: 0.8109 - val_recall: 0.7585 - val_accuracy: 0.5280 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 0.1766 - precision: 0.8648 - recall: 0.8543 - accuracy: 0.5167 - val_loss: 0.2365 - val_precision: 0.7877 - val_recall: 0.7762 - val_accuracy: 0.5263 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 0.1740 - precision: 0.8571 - recall: 0.8622 - accuracy: 0.4949 - val_loss: 0.2311 - val_precision: 0.8232 - val_recall: 0.7261 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 0.1700 - precision: 0.8643 - recall: 0.8695 - accuracy: 0.5113 - val_loss: 0.2316 - val_precision: 0.8135 - val_recall: 0.7282 - val_accuracy: 0.5263 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 0.1657 - precision: 0.8678 - recall: 0.8718 - accuracy: 0.5150 - val_loss: 0.2272 - val_precision: 0.8017 - val_recall: 0.7569 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 0.1620 - precision: 0.8729 - recall: 0.8698 - accuracy: 0.5093 - val_loss: 0.2258 - val_precision: 0.8059 - val_recall: 0.7449 - val_accuracy: 0.5085 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 0.1604 - precision: 0.8754 - recall: 0.8700 - accuracy: 0.5203 - val_loss: 0.2274 - val_precision: 0.8003 - val_recall: 0.7590 - val_accuracy: 0.5322 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 0.1582 - precision: 0.8723 - recall: 0.8698 - accuracy: 0.5045 - val_loss: 0.2322 - val_precision: 0.8042 - val_recall: 0.7606 - val_accuracy: 0.5085 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 0.1546 - precision: 0.8748 - recall: 0.8785 - accuracy: 0.5158 - val_loss: 0.2294 - val_precision: 0.8099 - val_recall: 0.7444 - val_accuracy: 0.5008 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 0.1524 - precision: 0.8792 - recall: 0.8782 - accuracy: 0.5325 - val_loss: 0.2278 - val_precision: 0.7969 - val_recall: 0.7533 - val_accuracy: 0.5246 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 0.1530 - precision: 0.8771 - recall: 0.8750 - accuracy: 0.5398 - val_loss: 0.2270 - val_precision: 0.8030 - val_recall: 0.7334 - val_accuracy: 0.5229 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 0.1491 - precision: 0.8800 - recall: 0.8762 - accuracy: 0.5147 - val_loss: 0.2387 - val_precision: 0.8037 - val_recall: 0.7110 - val_accuracy: 0.5373 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.1479 - precision: 0.8814 - recall: 0.8767 - accuracy: 0.5203 - val_loss: 0.2322 - val_precision: 0.8117 - val_recall: 0.7240 - val_accuracy: 0.4881 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 0.1438 - precision: 0.8895 - recall: 0.8812 - accuracy: 0.5215 - val_loss: 0.2314 - val_precision: 0.7997 - val_recall: 0.7454 - val_accuracy: 0.5356 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 0.1426 - precision: 0.8837 - recall: 0.8819 - accuracy: 0.5345 - val_loss: 0.2367 - val_precision: 0.8037 - val_recall: 0.7282 - val_accuracy: 0.5136 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 0.1411 - precision: 0.8861 - recall: 0.8900 - accuracy: 0.5223 - val_loss: 0.2324 - val_precision: 0.8052 - val_recall: 0.7287 - val_accuracy: 0.5161 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af31ce80>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some callbacks to improve training.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=10)\n",
    "\n",
    "# Define modifiable training hyperparameters.\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Fit the model to the training data.\n",
    "model.fit(x=X_train,\n",
    "          y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 53ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Choose timesteps to plot\n",
    "timestep = 4\n",
    "\n",
    "# Extract the data for the chosen timesteps from the tensor\n",
    "data1 = y_pred[timestep, 0, 0, :, :] > .5\n",
    "data2 = y_test[timestep, 0, 0, :, :]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plot the data in each subplot\n",
    "sns.heatmap(data1, cmap='viridis', vmin=-1, vmax=10, linewidths=0.5, linecolor='grey', annot=False, ax=ax1)\n",
    "sns.heatmap(data2, cmap='viridis', vmin=-1, vmax=10, linewidths=0.5, linecolor='grey', annot=False, ax=ax2)\n",
    "\n",
    "# Set the plot titles and axis labels\n",
    "ax1.set_title(f'Earthquake magnitudes at timestep {timestep}')\n",
    "ax1.set_xlabel('Longitude bin')\n",
    "ax1.set_ylabel('Latitude bin')\n",
    "\n",
    "ax2.set_title(f'Earthquake magnitudes at timestep {timestep}')\n",
    "ax2.set_xlabel('Longitude bin')\n",
    "ax2.set_ylabel('Latitude bin')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'M<6'), Text(0, 1.5, 'M>=6')]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p = sns.heatmap(confusion_matrix(np.array(y_test).flatten(), y_pred.flatten() >= 0.5), annot=True, fmt='g')\n",
    "p.set_xlabel(\"Predicted\")\n",
    "p.set_ylabel(\"True\")\n",
    "p.xaxis.set_ticklabels(['M<6', 'M>=6'], ha=\"center\", va=\"center\")\n",
    "p.yaxis.set_ticklabels(['M<6', 'M>=6'], rotation=0, va=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.909958\n",
      "Precision: 0.794218\n",
      "Recall: 0.742055\n",
      "F1 score: 0.767251\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         M<6       0.94      0.95      0.94      7552\n",
      "        M>=6       0.79      0.74      0.77      1888\n",
      "\n",
      "    accuracy                           0.91      9440\n",
      "   macro avg       0.87      0.85      0.86      9440\n",
      "weighted avg       0.91      0.91      0.91      9440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(np.array(y_test).flatten(), y_pred.flatten() >= 0.5)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(np.array(y_test).flatten(), y_pred.flatten() >= 0.5)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(np.array(y_test).flatten(), y_pred.flatten() >= 0.5)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(np.array(y_test).flatten(), y_pred.flatten() >= 0.5)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "class_names = ['M<6', 'M>=6']\n",
    "\n",
    "print(classification_report(np.array(y_test).flatten(), y_pred.flatten() >= 0.5, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hereafter is mainly redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat = pd.read_excel(\"/Users/jurrienboogert/Downloads/emdat_public_2023_02_08_query_uid-jHccdA.xlsx\", header=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emdat[emdat['Disaster Type'] == 'Earthquake'][['Dis Mag Value']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file='/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/NOAA/earthquakes-2023-02-11_10-24-26_+0100.tsv'\n",
    "NOAA=pd.read_table(tsv_file,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag']\n",
    "\n",
    "plt.hist(NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'], bins=80)\n",
    "plt.show()\n",
    "\n",
    "NOAA[(NOAA['Total Deaths'] > 0) | (NOAA['Total Damage ($Mil)'] > 0)]['Mag'].quantile(.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(NOAA[NOAA['Year'] > 2010][['year', 'month', 'day', 'hour', 'minute']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk1.csv\", low_memory=False)\n",
    "chunk2 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk2.csv\", low_memory=False)\n",
    "chunk3 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk3.csv\", low_memory=False)\n",
    "chunk4 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk4.csv\", low_memory=False)\n",
    "chunk5 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk5.csv\", low_memory=False)\n",
    "chunk6 = pd.read_csv(\"/Users/jurrienboogert/Documents/DATA_SCIENCE_AND_SOCIETY/THESIS/datasets/STEAD/chunk6.csv\", low_memory=False)\n",
    "\n",
    "chunks = pd.concat([chunk2,chunk3,chunk4,chunk5,chunk6], ignore_index=True)\n",
    "chunks['trace_start_time'] = pd.to_datetime(chunks['trace_start_time'])\n",
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "chunks = chunks.sort_values('source_origin_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset = chunks[(chunks['source_longitude'] >= 19) & (chunks['source_longitude'] <= 30) & (chunks['source_latitude'] >= 34) & (chunks['source_latitude'] <= 44) & (chunks['source_origin_time'] <= '2015-6-25 03:14:47.900')]\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "plt.scatter(subset.source_origin_time, subset.source_magnitude, s=1)\n",
    "plt.ylabel('Magnitude')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(subset.source_magnitude, bins=48)\n",
    "plt.xlabel('magnitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a histogram of the longitude values\n",
    "plt.hist(subset.source_latitude, bins=360)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Longitude')\n",
    "plt.show()\n",
    "\n",
    "# create a histogram of the latitude values\n",
    "plt.hist(subset.source_longitude, bins=360)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Number of Earthquakes')\n",
    "plt.title('Histogram of Earthquakes by Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "\n",
    "# Load earthquake data\n",
    "df = subset[chunks['source_magnitude'] >= 0].sort_values('source_magnitude')\n",
    "\n",
    "# Extract latitude and longitude columns\n",
    "latitudes = df['source_latitude']\n",
    "longitudes = df['source_longitude']\n",
    "magnitudes = df['source_magnitude']\n",
    "\n",
    "# Set up map projection\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "map = Basemap(projection='merc', lat_0=0, lon_0=0, resolution='l',\n",
    "              llcrnrlon=19, llcrnrlat=33, urcrnrlon=30, urcrnrlat=43)\n",
    "\n",
    "# Draw coastlines, countries, and states\n",
    "#map.drawcoastlines(color='gray')\n",
    "map.fillcontinents(color='lightgray', lake_color='white')\n",
    "map.drawmapboundary(fill_color='white')\n",
    "\n",
    "# Draw parallels and meridians\n",
    "map.drawparallels(range(-90, 90, 1), linewidth=0.5, labels=[1, 0, 0, 0])\n",
    "meridians = map.drawmeridians(range(-180, 180, 1), linewidth=0.5, labels=[0, 0, 0, 1])\n",
    "\n",
    "for m in meridians:\n",
    "    try:\n",
    "        meridians[m][1][0].set_rotation(45)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Convert latitude and longitude to map coordinates\n",
    "x, y = map(longitudes, latitudes)\n",
    "\n",
    "# Plot earthquake magnitudes as circles on the map\n",
    "map.scatter(x, y, s=np.exp(magnitudes)/50, c=magnitudes, cmap='plasma', alpha=1)\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(label='Magnitude')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Earthquake Magnitudes')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# group the data by date and location to create the input data for Prophet\n",
    "data = chunks.groupby([pd.Grouper(key='source_origin_time', freq='D'), 'source_latitude', 'source_longitude']).size().reset_index(name='count')\n",
    "\n",
    "# rename columns for use with Prophet\n",
    "data = data.rename(columns={'source_origin_time': 'ds', 'count': 'y'})\n",
    "\n",
    "# create a Prophet model and fit the data\n",
    "model = Prophet()\n",
    "model.fit(data)\n",
    "\n",
    "# create a future dataframe with predictions for the next 365 days\n",
    "future = model.make_future_dataframe(periods=7)\n",
    "\n",
    "# predict the number of earthquakes for the future dates\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# plot the forecast\n",
    "fig = model.plot(forecast, xlabel='Date', ylabel='Number of Earthquakes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chunks['source_origin_time'], chunks['source_magnitude'], 'ro', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[['source_magnitude', 'trace_start_time', 'source_origin_time', 'receiver_latitude', 'receiver_longitude', 'source_latitude', 'source_longitude']].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[(chunks['source_origin_time'].dt.year == 2007) & (chunks['source_origin_time'].dt.month == 8) & (chunks['source_origin_time'].dt.day == 17) & (chunks['source_origin_time'].dt.hour == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[chunks['source_magnitude'] > 6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA[(NOAA['Year'] > 1983) & (NOAA['Deaths'] > 0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"/Users/jurrienboogert/Downloads/2023.csv\", header=None, names=['ID', 'YEAR/MONTH/DAY', 'ELEMENT', 'DATA VALUE', 'M-FLAG', 'Q-FLAG', 'S-FLAG', 'OBS-TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dly = pd.read_fwf('/Users/jurrienboogert/Downloads/ghcnd_gsn/ghcnd_gsn/USW00013782.dly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = chunks.sample(1)[['source_latitude', 'source_longitude']]\n",
    "source.iloc[0,0], source.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "# Set time period\n",
    "start = datetime(1984, 9, 7, 2)\n",
    "end = datetime(1984, 9, 7, 3)\n",
    "\n",
    "# Create Point for Vancouver, BC\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 200000\n",
    "Point.max_count = 6\n",
    "Point.weight_dist = .6\n",
    "receiver = Point(source.iloc[0,0], source.iloc[0,1])\n",
    "# Get daily data for 2018\n",
    "data = Hourly(receiver, start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks['source_origin_time'] = pd.to_datetime(chunks['source_origin_time'])\n",
    "year = chunks['source_origin_time'].dt.year.fillna(0).astype('int')\n",
    "month = chunks['source_origin_time'].dt.month.fillna(0).astype('int')\n",
    "day = chunks['source_origin_time'].dt.day.fillna(0).astype('int')\n",
    "hour = chunks['source_origin_time'].dt.hour.fillna(0).astype('int')\n",
    "\n",
    "chunks.source_latitude\n",
    "chunks.source_longitude\n",
    "\n",
    "chunks.receiver_latitude\n",
    "chunks.receiver_longitude\n",
    "chunks.receiver_elevation_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "temp_source = []\n",
    "rhum_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "rhum_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i], hour[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Hourly(source, start, end).fetch()['temp'][0])\n",
    "    rhum_source.append(Hourly(source, start, end).fetch()['rhum'][0])\n",
    "    pres_source.append(Hourly(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "temp_source = []\n",
    "pres_source = []\n",
    "temp_receiver = []\n",
    "pres_receiver = []\n",
    "counter = 0\n",
    "\n",
    "Point.method = 'nearest'\n",
    "Point.radius = 2000000\n",
    "Point.max_count = 10\n",
    "Point.weight_dist = .6\n",
    "\n",
    "for i in range(235426,len(chunks)):\n",
    "    counter += 1\n",
    "    start = datetime(year[i], month[i], day[i])\n",
    "    end = start\n",
    "\n",
    "    source = Point(chunks.source_latitude[i], chunks.source_longitude[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_source.append(Daily(source, start, end).fetch()['tavg'][0])\n",
    "    pres_source.append(Daily(source, start, end).fetch()['pres'][0])\n",
    "\n",
    "    Point.alt_range = chunks.receiver_elevation_m[i]\n",
    "    Point.adapt_temp = True\n",
    "    receiver = Point(chunks.receiver_latitude[i], chunks.receiver_longitude[i], chunks.receiver_elevation_m[i])\n",
    "    # Get daily data for 2018\n",
    "    temp_receiver.append(Daily(receiver, start, end).fetch()['tavg'][0])\n",
    "    pres_receiver.append(Daily(receiver, start, end).fetch()['pres'][0])\n",
    "    if counter % 10 == 0:\n",
    "        print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8acd2a4c40bb06440d03e583eeea35c6596324a3385dafe16353bbc1939be192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
