{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM/GRU/MLP univariate/multivariate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Depth</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1973-01-03 06:47:47.800</th>\n",
       "      <td>145.6740</td>\n",
       "      <td>16.8090</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177.000</td>\n",
       "      <td>1973-01-03 06:47:47.800</td>\n",
       "      <td>94891667000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973-01-05 05:31:05.800</th>\n",
       "      <td>140.8660</td>\n",
       "      <td>33.4750</td>\n",
       "      <td>4.5</td>\n",
       "      <td>56.000</td>\n",
       "      <td>1973-01-05 05:31:05.800</td>\n",
       "      <td>95059865000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973-01-05 11:48:37.500</th>\n",
       "      <td>140.9150</td>\n",
       "      <td>33.1600</td>\n",
       "      <td>3.9</td>\n",
       "      <td>33.000</td>\n",
       "      <td>1973-01-05 11:48:37.500</td>\n",
       "      <td>95082517000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973-01-05 13:39:50.400</th>\n",
       "      <td>146.8230</td>\n",
       "      <td>48.1620</td>\n",
       "      <td>5.1</td>\n",
       "      <td>430.000</td>\n",
       "      <td>1973-01-05 13:39:50.400</td>\n",
       "      <td>95089190000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973-01-06 09:58:32.900</th>\n",
       "      <td>144.5900</td>\n",
       "      <td>12.2760</td>\n",
       "      <td>4.7</td>\n",
       "      <td>32.000</td>\n",
       "      <td>1973-01-06 09:58:32.900</td>\n",
       "      <td>95162312000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30 22:42:08.690</th>\n",
       "      <td>160.3568</td>\n",
       "      <td>52.4068</td>\n",
       "      <td>4.7</td>\n",
       "      <td>10.000</td>\n",
       "      <td>2022-12-30 22:42:08.690</td>\n",
       "      <td>1672440128000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 03:27:38.307</th>\n",
       "      <td>146.1845</td>\n",
       "      <td>19.1089</td>\n",
       "      <td>4.0</td>\n",
       "      <td>89.055</td>\n",
       "      <td>2022-12-31 03:27:38.307</td>\n",
       "      <td>1672457258000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 15:24:22.787</th>\n",
       "      <td>143.7703</td>\n",
       "      <td>12.1600</td>\n",
       "      <td>4.7</td>\n",
       "      <td>10.000</td>\n",
       "      <td>2022-12-31 15:24:22.787</td>\n",
       "      <td>1672500262000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 16:29:24.958</th>\n",
       "      <td>151.5686</td>\n",
       "      <td>45.2726</td>\n",
       "      <td>4.3</td>\n",
       "      <td>39.558</td>\n",
       "      <td>2022-12-31 16:29:24.958</td>\n",
       "      <td>1672504164000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31 19:38:06.265</th>\n",
       "      <td>145.0542</td>\n",
       "      <td>13.9106</td>\n",
       "      <td>4.1</td>\n",
       "      <td>141.638</td>\n",
       "      <td>2022-12-31 19:38:06.265</td>\n",
       "      <td>1672515486000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69446 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Longitude  Latitude  Magnitude    Depth  \\\n",
       "Time                                                               \n",
       "1973-01-03 06:47:47.800   145.6740   16.8090        4.0  177.000   \n",
       "1973-01-05 05:31:05.800   140.8660   33.4750        4.5   56.000   \n",
       "1973-01-05 11:48:37.500   140.9150   33.1600        3.9   33.000   \n",
       "1973-01-05 13:39:50.400   146.8230   48.1620        5.1  430.000   \n",
       "1973-01-06 09:58:32.900   144.5900   12.2760        4.7   32.000   \n",
       "...                            ...       ...        ...      ...   \n",
       "2022-12-30 22:42:08.690   160.3568   52.4068        4.7   10.000   \n",
       "2022-12-31 03:27:38.307   146.1845   19.1089        4.0   89.055   \n",
       "2022-12-31 15:24:22.787   143.7703   12.1600        4.7   10.000   \n",
       "2022-12-31 16:29:24.958   151.5686   45.2726        4.3   39.558   \n",
       "2022-12-31 19:38:06.265   145.0542   13.9106        4.1  141.638   \n",
       "\n",
       "                                      timestamp       unix_timestamp  \n",
       "Time                                                                  \n",
       "1973-01-03 06:47:47.800 1973-01-03 06:47:47.800    94891667000000000  \n",
       "1973-01-05 05:31:05.800 1973-01-05 05:31:05.800    95059865000000000  \n",
       "1973-01-05 11:48:37.500 1973-01-05 11:48:37.500    95082517000000000  \n",
       "1973-01-05 13:39:50.400 1973-01-05 13:39:50.400    95089190000000000  \n",
       "1973-01-06 09:58:32.900 1973-01-06 09:58:32.900    95162312000000000  \n",
       "...                                         ...                  ...  \n",
       "2022-12-30 22:42:08.690 2022-12-30 22:42:08.690  1672440128000000000  \n",
       "2022-12-31 03:27:38.307 2022-12-31 03:27:38.307  1672457258000000000  \n",
       "2022-12-31 15:24:22.787 2022-12-31 15:24:22.787  1672500262000000000  \n",
       "2022-12-31 16:29:24.958 2022-12-31 16:29:24.958  1672504164000000000  \n",
       "2022-12-31 19:38:06.265 2022-12-31 19:38:06.265  1672515486000000000  \n",
       "\n",
       "[69446 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Choose frequency, binsize, longitude, latitude\n",
    "freq = 'w'\n",
    "binsize = 2\n",
    "longitude_W = 134 # minimum is 134\n",
    "longitude_E = 174 # maximum is 174\n",
    "latitude_S = 10 # minimum is 10\n",
    "latitude_N = 60 # minimum is 60\n",
    "\n",
    "# load earthquake data for defined area\n",
    "data = pd.read_csv('data/Japan_10_60_134_174_1973_2023_V2.csv', index_col=0)\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "data = data[(data.Longitude >= longitude_W) & (data.Longitude <= longitude_E) & (data.Latitude >= latitude_S) & (data.Latitude <= latitude_N)]\n",
    "data.set_index('Time', inplace=True)\n",
    "df = data.sort_index()\n",
    "df['timestamp'] = df.index.map(pd.Timestamp)\n",
    "df['unix_timestamp'] = df['timestamp'].dt.floor('s').astype(int)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data in train, val, test\n",
    "\n",
    "# Activate when only forecasting one region\n",
    "# matrix = np.reshape(matrix, (matrix.shape[0], matrix.shape[1], 1))\n",
    "\n",
    "train, val_test = train_test_split(df, test_size=.3, shuffle=False, random_state=43)\n",
    "val, test = train_test_split(val_test, test_size=.5, shuffle=False, random_state=43)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets from timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import tensorflow as tf\n",
    "\n",
    "def dataset_generator(data, seq_length, cutoff):\n",
    "\n",
    "  input_data = data # data[:-seq_length]\n",
    "  targets = data[seq_length:]\n",
    "  dataset = timeseries_dataset_from_array(input_data, (targets >= cutoff).astype(int), sequence_length=seq_length, sampling_rate=1, sequence_stride=1, shuffle=False, batch_size=len(data))\n",
    "  \"\"\"\n",
    "  for batch in dataset:\n",
    "    inputs, targets = batch\n",
    "    assert np.array_equal(inputs[0], data[:seq_length])  # First sequence: steps [0-9]\n",
    "    assert np.array_equal(targets[0], data[seq_length])  # Corresponding target: step 10\n",
    "    \"\"\"\n",
    "  return dataset\n",
    "\n",
    "# Set lookback timewindow\n",
    "timewindow = 365\n",
    "cutoff = 4.5\n",
    "\n",
    "train_dataset = dataset_generator(train, timewindow, cutoff)\n",
    "val_dataset = dataset_generator(val, timewindow, cutoff)\n",
    "test_dataset = dataset_generator(test, timewindow, cutoff)\n",
    "\n",
    "# Create train set\n",
    "for batch in train_dataset:\n",
    "    X_train, y_train = batch\n",
    "\n",
    "# y_train = tf.reshape(y_train, shape=[y_train.shape[0], 1, y_train.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_train = tf.cast(tf.reduce_max(y_train, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create validation set\n",
    "for batch in val_dataset:\n",
    "    X_val, y_val = batch\n",
    "\n",
    "# y_val = tf.reshape(y_val, shape=[y_val.shape[0], 1, y_val.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_val = tf.cast(tf.reduce_max(y_val, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create test set\n",
    "for batch in test_dataset:\n",
    "    X_test, y_test = batch\n",
    "\n",
    "# y_test = tf.reshape(y_test, shape=[y_test.shape[0], 1, y_test.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_test = tf.cast(tf.reduce_max(y_test, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "################################# Use for MLP\n",
    "# Flatten 1 and 2 dimensions of X's for multivariate MLP\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0],-1))\n",
    "# X_val = np.reshape(X_val, (X_val.shape[0],-1))\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0],-1))\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:103\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:487\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    484\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[1;32m    485\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[0;32m--> 487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    488\u001b[0m     element,\n\u001b[1;32m    489\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for                          Longitude  Latitude  Magnitude  Depth  \\\nTime                                                             \n1973-01-03 06:47:47.800    145.674    16.809        4.0  177.0   \n1973-01-05 05:31:05.800    140.866    33.475        4.5   56.0   \n1973-01-05 11:48:37.500    140.915    33.160        3.9   33.0   \n1973-01-05 13:39:50.400    146.823    48.162        5.1  430.0   \n1973-01-06 09:58:32.900    144.590    12.276        4.7   32.0   \n...                            ...       ...        ...    ...   \n2011-05-16 21:14:05.890    141.760    38.202        4.5   53.7   \n2011-05-16 22:14:23.340    142.145    41.618        4.5   70.0   \n2011-05-17 04:54:50.420    142.167    39.468        4.6   55.7   \n2011-05-17 05:28:30.830    143.857    26.932        4.1   10.0   \n2011-05-17 06:52:39.440    141.573    35.636        4.2   35.0   \n\n                                      timestamp       unix_timestamp  \nTime                                                                  \n1973-01-03 06:47:47.800 1973-01-03 06:47:47.800    94891667000000000  \n1973-01-05 05:31:05.800 1973-01-05 05:31:05.800    95059865000000000  \n1973-01-05 11:48:37.500 1973-01-05 11:48:37.500    95082517000000000  \n1973-01-05 13:39:50.400 1973-01-05 13:39:50.400    95089190000000000  \n1973-01-06 09:58:32.900 1973-01-06 09:58:32.900    95162312000000000  \n...                                         ...                  ...  \n2011-05-16 21:14:05.890 2011-05-16 21:14:05.890  1305580445000000000  \n2011-05-16 22:14:23.340 2011-05-16 22:14:23.340  1305584063000000000  \n2011-05-17 04:54:50.420 2011-05-17 04:54:50.420  1305608090000000000  \n2011-05-17 05:28:30.830 2011-05-17 05:28:30.830  1305610110000000000  \n2011-05-17 06:52:39.440 2011-05-17 06:52:39.440  1305615159000000000  \n\n[48612 rows x 6 columns] with type DataFrame",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m future_timesteps \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m     15\u001b[0m timewindow \u001b[39m=\u001b[39m lookback_window \u001b[39m+\u001b[39m future_timesteps\n\u001b[0;32m---> 17\u001b[0m train_dataset \u001b[39m=\u001b[39m dataset_generator(train, timewindow, cutoff)\n\u001b[1;32m     18\u001b[0m val_dataset \u001b[39m=\u001b[39m dataset_generator(val, timewindow, cutoff)\n\u001b[1;32m     19\u001b[0m test_dataset \u001b[39m=\u001b[39m dataset_generator(test, timewindow, cutoff)\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mdataset_generator\u001b[0;34m(data, seq_length, cutoff)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdataset_generator\u001b[39m(data, seq_length, cutoff):\n\u001b[1;32m      6\u001b[0m   input_data \u001b[39m=\u001b[39m data\n\u001b[0;32m----> 7\u001b[0m   dataset \u001b[39m=\u001b[39m timeseries_dataset_from_array(input_data, targets\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, sequence_length\u001b[39m=\u001b[39;49mseq_length, sampling_rate\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, sequence_stride\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(data))\n\u001b[1;32m      9\u001b[0m   \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/keras/utils/timeseries_dataset.py:245\u001b[0m, in \u001b[0;36mtimeseries_dataset_from_array\u001b[0;34m(data, targets, sequence_length, sequence_stride, sampling_rate, batch_size, shuffle, seed, start_index, end_index)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39m# For each initial window position, generates indices of the window elements\u001b[39;00m\n\u001b[1;32m    234\u001b[0m indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip(\n\u001b[1;32m    235\u001b[0m     (tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mrange(\u001b[39mlen\u001b[39m(start_positions)), positions_ds)\n\u001b[1;32m    236\u001b[0m )\u001b[39m.\u001b[39mmap(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    243\u001b[0m )\n\u001b[0;32m--> 245\u001b[0m dataset \u001b[39m=\u001b[39m sequences_from_indices(data, indices, start_index, end_index)\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip(\n\u001b[1;32m    248\u001b[0m         (tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mrange(\u001b[39mlen\u001b[39m(start_positions)), positions_ds)\n\u001b[1;32m    249\u001b[0m     )\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    250\u001b[0m         \u001b[39mlambda\u001b[39;00m i, positions: positions[i],\n\u001b[1;32m    251\u001b[0m         num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    252\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/keras/utils/timeseries_dataset.py:270\u001b[0m, in \u001b[0;36msequences_from_indices\u001b[0;34m(array, indices_ds, start_index, end_index)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msequences_from_indices\u001b[39m(array, indices_ds, start_index, end_index):\n\u001b[0;32m--> 270\u001b[0m     dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensors(array[start_index:end_index])\n\u001b[1;32m    271\u001b[0m     dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip((dataset\u001b[39m.\u001b[39mrepeat(), indices_ds))\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    272\u001b[0m         \u001b[39mlambda\u001b[39;00m steps, inds: tf\u001b[39m.\u001b[39mgather(steps, inds),\n\u001b[1;32m    273\u001b[0m         num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:734\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensors\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    699\u001b[0m   \u001b[39m\"\"\"Creates a `Dataset` with a single element, comprising the given tensors.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \n\u001b[1;32m    701\u001b[0m \u001b[39m  `from_tensors` produces a dataset containing only a single element. To slice\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4688\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m   4686\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   4687\u001b[0m   \u001b[39m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4688\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[1;32m   4689\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m   4690\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure, element)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:108\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    103\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    106\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 108\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, sparse_tensor\u001b[39m.\u001b[39mSparseTensorSpec):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1629\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1631\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1635\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1637\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1640\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1641\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "import tensorflow as tf\n",
    "\n",
    "def dataset_generator(data, seq_length, cutoff):\n",
    "\n",
    "  input_data = data\n",
    "  dataset = timeseries_dataset_from_array(input_data, targets=None, sequence_length=seq_length, sampling_rate=1, sequence_stride=1, shuffle=False, batch_size=len(data))\n",
    "\n",
    "  return dataset\n",
    "\n",
    "# Set lookback timewindow\n",
    "lookback_window = 52\n",
    "cutoff = 4.5\n",
    "future_timesteps = 4\n",
    "timewindow = lookback_window + future_timesteps\n",
    "\n",
    "train_dataset = dataset_generator(train, timewindow, cutoff)\n",
    "val_dataset = dataset_generator(val, timewindow, cutoff)\n",
    "test_dataset = dataset_generator(test, timewindow, cutoff)\n",
    "\n",
    "# Create train set\n",
    "for batch in train_dataset:\n",
    "    X_train = batch[:,:-future_timesteps,:]\n",
    "    y_train = batch[:,-future_timesteps:,:] >= cutoff\n",
    "    y_train = np.max(y_train, axis=1)\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], 1, y_train.shape[1])).astype(int)\n",
    "\n",
    "# y_train = tf.reshape(y_train, shape=[y_train.shape[0], 1, y_train.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_train = tf.cast(tf.reduce_max(y_train, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create validation set\n",
    "for batch in val_dataset:\n",
    "    X_val = batch[:,:-future_timesteps,:]\n",
    "    y_val = batch[:,-future_timesteps:,:] >= cutoff\n",
    "    y_val = np.max(y_val, axis=1)\n",
    "    y_val = np.reshape(y_val, (y_val.shape[0], 1, y_val.shape[1])).astype(int)\n",
    "\n",
    "# y_val = tf.reshape(y_val, shape=[y_val.shape[0], 1, y_val.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_val = tf.cast(tf.reduce_max(y_val, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "# Create test set\n",
    "for batch in test_dataset:\n",
    "    X_test = batch[:,:-future_timesteps,:]\n",
    "    y_test = batch[:,-future_timesteps:,:] >= cutoff\n",
    "    y_test = np.max(y_test, axis=1)\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], 1, y_test.shape[1])).astype(int)\n",
    "\n",
    "# y_test = tf.reshape(y_test, shape=[y_test.shape[0], 1, y_test.shape[1]])\n",
    "\n",
    "# Collapse the depth dimension and converts all non-zero values to 1 and zero values to 0\n",
    "# y_test = tf.cast(tf.reduce_max(y_test, axis=2, keepdims=True) > 0, dtype=tf.int32)\n",
    "\n",
    "################################# Use for MLP\n",
    "# Flatten 1 and 2 dimensions of X's for multivariate MLP\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0],-1))\n",
    "# X_val = np.reshape(X_val, (X_val.shape[0],-1))\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0],-1))\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct LSTM (univariate/multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Flatten, Input, TimeDistributed, Dropout, RepeatVector, BatchNormalization\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(512, activation='LeakyReLU',\n",
    "               return_sequences=False,\n",
    "               input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "               kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),\n",
    "               bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),\n",
    "               # dropout=0.4,\n",
    "               ))\n",
    "model.add(RepeatVector(y_train.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(512, activation='LeakyReLU', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Dense(X_train.shape[2], activation=\"sigmoid\")))\n",
    "\"\"\"\n",
    "model.add(Dense(12, input_dim=(X_train.shape[1]), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\"\"\"\n",
    "\n",
    "# model.add(Dense(1024, activation='LeakyReLU'))\n",
    "# model.add(Dense(80, activation='sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.BinaryFocalCrossentropy( apply_class_balancing=True, alpha=.8, gamma=1)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "model.build(input_shape=X_train.shape)\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# fit model\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    batch_size=128,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    callbacks=[callback],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# y_pred = scaler.inverse_transform(y_pred)\n",
    "# y_test = scaler.inverse_transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(np.array(y_test).flatten(), y_pred.flatten() >= .5)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(np.array(y_test).flatten(), y_pred.flatten() >= .5)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(np.array(y_test).flatten(), y_pred.flatten() >= .5)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(np.array(y_test).flatten(), y_pred.flatten() >= .5)\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "class_names = ['M<4.5', 'M>=4.5']\n",
    "\n",
    "print(classification_report(np.array(y_test).flatten(), y_pred.flatten() >= .5, target_names=class_names))\n",
    "\n",
    "# Calculate the proportion of the majority per row, column combination over all batches\n",
    "majority_prop = np.mean(test >= cutoff, axis=0)[:]\n",
    "\n",
    "# Calculate the complement for values lower than 0.5\n",
    "majority_prop = np.where(majority_prop < 0.5, 1 - majority_prop, majority_prop)\n",
    "zeroR = majority_prop.mean()\n",
    "\n",
    "print(\"zeroR:\", round(zeroR,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "p = sns.heatmap(confusion_matrix(np.array(y_test).flatten(), y_pred.flatten() >= .5), annot=True, fmt='g')\n",
    "p.set_xlabel(\"Predicted\")\n",
    "p.set_ylabel(\"True\")\n",
    "p.xaxis.set_ticklabels(['M<4.5', 'M>=4.5'], ha=\"center\", va=\"center\")\n",
    "p.yaxis.set_ticklabels(['M<4.5', 'M>=4.5'], rotation=0, va=\"center\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_ROC_AUC(X,y):\n",
    "\n",
    "    # Use the trained model to predict the class probabilities for the validation set\n",
    "    y_prob = model.predict(X)\n",
    "    # y_pred = scaler.inverse_transform(y_prob)\n",
    "    # y_test = scaler.inverse_transform(y)\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(np.array(y_test).flatten(), y_pred.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
    "            ''.format(roc_auc), color='blue', linewidth=1)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    # plt.savefig(savefig)\n",
    "    plt.show()\n",
    "\n",
    "plot_ROC_AUC(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(np.array(y_test).flatten(), y_pred.flatten())\n",
    "auc = auc(recall, precision)\n",
    "\n",
    "# Calculate the proportion of the majority per row, column combination over all batches\n",
    "majority_prop = np.mean(test >= cutoff, axis=0)[:]\n",
    "\n",
    "# Calculate the complement for values lower than 0.5\n",
    "majority_prop = np.where(majority_prop < 0.5, 1 - majority_prop, majority_prop)\n",
    "zeroR = majority_prop.mean()\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = (np.array(y_test).flatten() == 1).sum() / len(np.array(y_test).flatten())\n",
    "plt.plot([0, 1], [no_skill,no_skill], linestyle='--', label='class 1 = %.3f' % (no_skill))\n",
    "plt.plot(recall, precision, label='PR_curve, AUC = %.3f' % (auc))\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predicted against true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape data to grid\n",
    "y_pred = np.reshape(y_pred, (y_pred.shape[0], tensor.shape[1], tensor.shape[2]))\n",
    "y_test= np.reshape(y_test, (y_test.shape[0], tensor.shape[1], tensor.shape[2]))\n",
    "\n",
    "# Choose timesteps to plot\n",
    "timestep = 1\n",
    "\n",
    "\n",
    "# Extract the data for the chosen timesteps from the tensor\n",
    "data1 = y_pred[timestep, :, :] >= 0.5\n",
    "data2 = y_test[timestep, :, :]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(9, 4.5))\n",
    "\n",
    "# Plot the data in each subplot\n",
    "sns.heatmap(data1, cmap='viridis', vmin=0, vmax=1, linewidths=0.5, linecolor='grey', annot=False, ax=ax1)\n",
    "sns.heatmap(data2, cmap='viridis', vmin=0, vmax=1, linewidths=0.5, linecolor='grey', annot=False, ax=ax2)\n",
    "\n",
    "# Set the plot titles and axis labels\n",
    "ax1.set_title(f'Predicted {cutoff} magnitude at timestep {timestep}')\n",
    "ax1.set_xlabel('Longitude bin')\n",
    "ax1.set_ylabel('Latitude bin')\n",
    "\n",
    "ax2.set_title(f'True {cutoff} magnitude at timestep {timestep}')\n",
    "ax2.set_xlabel('Longitude bin')\n",
    "ax2.set_ylabel('Latitude bin')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8acd2a4c40bb06440d03e583eeea35c6596324a3385dafe16353bbc1939be192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
